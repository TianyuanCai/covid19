{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lib import get_data\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = ['date', 'county', 'state', 'fips', 'cases', 'deaths', \n",
    "                'heatIndexMin', 'heatIndexAvg', 'heatIndexMax','relHumMin',\n",
    "                'relHumAvg', 'relHumMax',  'tempMin', 'tempAvg', 'tempMax', 'windChillMin', 'windChillAvg',\n",
    "                'windChillMax', 'windDirAvg', 'retail_and_recreation_percent_change_from_baseline',\n",
    "                'residential_percent_change_from_baseline',\n",
    "                'workplaces_percent_change_from_baseline',\n",
    "                'transit_stations_percent_change_from_baseline',\n",
    "                'grocery_and_pharmacy_percent_change_from_baseline',\n",
    "                'parks_percent_change_from_baseline', 'income_2018', 'pop_2018',\n",
    "                'pred_cases', 'pred_deaths']\n",
    "LEN = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_path = '../data/processed/time_series_all.csv'\n",
    "data = pd.read_csv(processed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize income and pop\n",
    "income_max = data['income_2018'].max()\n",
    "income_min = data['income_2018'].min()\n",
    "data['income_2018'] = (data['income_2018'] - income_min) / income_max\n",
    "\n",
    "pop_max = data['pop_2018'].max()\n",
    "pop_min = data['pop_2018'].min()\n",
    "data['pop_2018'] = (data['pop_2018']-pop_min)/pop_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10105518.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = data[data['date'] <= '2020-04-10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_train = subset[subset['date'] <= '2020-04-02']\n",
    "subset_test = subset[subset['date'] > '2020-04-02']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_data_cumulative(data, date_range=(0, 14), pred_day=21):\n",
    "    \"\"\"\n",
    "\n",
    "    :param date_range: integer measuring the number of days since first day w/ 10 cases\n",
    "    :param pred_day:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # ensure full coverage of interested dates\n",
    "    data = data[data['fips'].isin(data[data['days_since_10_cases'] == pred_day]['fips'].drop_duplicates())]\n",
    "\n",
    "    # filter for training and testing dates\n",
    "    data_x = data[data['days_since_10_cases'].between(date_range[0], date_range[1])]\n",
    "    data_grouped = data_x.groupby(['state', 'county', 'fips']).agg('mean').reset_index()  # todo play with granularity\n",
    "    data_x = data_x.drop(['days_since_10_cases'], axis=1)\n",
    "\n",
    "    # get change rate since last training date\n",
    "    data_y = data[data['days_since_10_cases'].isin([date_range[1], pred_day])][\n",
    "        ['fips', 'cases', 'deaths', 'days_since_10_cases']]\n",
    "    data_y = data_y.sort_values(['fips', 'days_since_10_cases']).reset_index(drop=True)\n",
    "    day_idx = np.where(data_y['days_since_10_cases'] == pred_day)[0]\n",
    "    data_y_new = data_y.iloc[day_idx, :].reset_index()\n",
    "    data_y_new = data_y_new.drop(['index','days_since_10_cases'], axis=1)\n",
    "    data_y_new['fips'] = data_y.loc[day_idx, 'fips'].reset_index(drop=True)\n",
    "    data_y_new = data_y_new.add_prefix('pred_')\n",
    "    \n",
    "    \n",
    "    data = data_x.merge(data_y_new, left_on='fips', right_on='pred_fips', how='inner')\n",
    "#     data_grouped = data_grouped.merge(data_y, left_on='fips', right_on=f'day_{pred_day}_cumulative_fips',\n",
    "#                                       how='inner')\n",
    "#     data_grouped = data_grouped.drop(f'day_{pred_day}_cumulative_fips', axis=1)\n",
    "    data = data.drop('pred_fips', axis=1)\n",
    "    \n",
    "    data = data[COLUMNS]\n",
    "    data = data.fillna(0)\n",
    "    data = data.drop(['county','state'],axis=1)\n",
    "#     data_grouped = data_grouped[COLUMNS]\n",
    "    return data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating training set\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 1\n",
    "end = 65\n",
    "\n",
    "X_train = np.array([])\n",
    "y_train = np.array([])\n",
    "\n",
    "for i in range(start, end):\n",
    "    df = get_model_data_cumulative(data, date_range=(i, i+LEN), pred_day=i+LEN+1)\n",
    "    if df.empty is False:\n",
    "        grouped_by_fips = df.groupby(['fips']).groups\n",
    "        count = 0\n",
    "        for fips in grouped_by_fips:\n",
    "            index = grouped_by_fips[fips]\n",
    "            train = np.expand_dims(df.iloc[index].drop(['date','fips'],axis=1).to_numpy(),axis=0)[:,:,:-1]\n",
    "#             train = np.expand_dims(df.iloc[index].drop(['date','fips'],axis=1)[['cases','pred_cases']].to_numpy(),axis=0)\n",
    "            \n",
    "            if i == 1:\n",
    "                X_train = train[:,:,:-1]\n",
    "#                 X_train = train[:,:,0]\n",
    "                y_train = train[:,:,-1].mean()\n",
    "#                 y_train = train[:,:,1].mean()\n",
    "            else:\n",
    "                X_train = np.vstack((X_train,  train[:,:,:-1]))\n",
    "                y_train =np.vstack((y_train, train[:,:,-1].mean()))\n",
    "# X_train = np.array(X_train)\n",
    "# y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 7, 2)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_model_data_cumulative(subset_train, date_range=(1, 1+LEN), pred_day=1+LEN+1)\n",
    "index = df.groupby(['fips']).groups[1017]\n",
    "df.iloc[index].drop(['date','fips'],axis=1)[['cases','pred_cases']].to_numpy(),axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2165, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN\n",
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4-tf\n",
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import backend\n",
    "\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.layers import Input, SimpleRNN, Embedding, Dense, TimeDistributed, GRU, \\\n",
    "                          Dropout, Bidirectional, Conv1D, BatchNormalization, RepeatVector, LSTM, MaxPooling1D\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(tf.keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store model\n",
    "def store_keras_model(model, model_name):\n",
    "    model_json = model.to_json() # serialize model to JSON\n",
    "    with open(\"./models/{}.json\".format(model_name), \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(\"./models/{}.h5\".format(model_name)) # serialize weights to HDF5\n",
    "    print(\"Saved model to disk\")\n",
    "    \n",
    "# Plot history\n",
    "def plot_training_history(history):\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1,len(loss)+1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Load model \n",
    "def load_keras_model(model_name):\n",
    "    # Load json and create model\n",
    "    json_file = open('./models/{}.json'.format(model_name), 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = tf.keras.models.model_from_json(loaded_model_json)\n",
    "    # Load weights into new model\n",
    "    model.load_weights(\"./models/{}.h5\".format(model_name))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "input0 = Input(shape=(7, 23), name='Input')\n",
    "lstm = LSTM(200, return_sequences=False,dropout=0.4)(input0)\n",
    "dense1 = Dense(128, activation = 'relu')(lstm)\n",
    "dropout1 = Dropout(0.5)(dense1)\n",
    "dense2 = Dense(64, activation = 'relu')(dropout1)\n",
    "dropout2 = Dropout(0.5)(dense2)\n",
    "output = Dense(1, activation = 'linear')(dropout2)\n",
    "# output = TimeDistributed(Dense(n_tags, activation='softmax'))(input3)\n",
    "\n",
    "model = Model(input0, output, name='model_RNN')\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_RNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 7, 23)]           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 200)               179200    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               25728     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 213,249\n",
      "Trainable params: 213,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1948 samples, validate on 217 samples\n",
      "Epoch 1/1000\n",
      "1948/1948 [==============================] - 1s 504us/sample - loss: 336001.1503 - mse: 336001.1562 - val_loss: 7444217.0000 - val_mse: 7444217.0000\n",
      "Epoch 2/1000\n",
      "1948/1948 [==============================] - 0s 7us/sample - loss: 335529.6808 - mse: 335529.6562 - val_loss: 7442175.5000 - val_mse: 7442175.5000\n",
      "Epoch 3/1000\n",
      "1948/1948 [==============================] - 0s 7us/sample - loss: 335020.8325 - mse: 335020.8125 - val_loss: 7440260.0000 - val_mse: 7440260.0000\n",
      "Epoch 4/1000\n",
      "1948/1948 [==============================] - 0s 7us/sample - loss: 334511.6729 - mse: 334511.6562 - val_loss: 7438125.5000 - val_mse: 7438125.5000\n",
      "Epoch 5/1000\n",
      "1948/1948 [==============================] - 0s 7us/sample - loss: 333919.0893 - mse: 333919.0938 - val_loss: 7435520.0000 - val_mse: 7435520.0000\n",
      "Epoch 6/1000\n",
      "1948/1948 [==============================] - 0s 7us/sample - loss: 332933.1154 - mse: 332933.1250 - val_loss: 7432192.5000 - val_mse: 7432192.5000\n",
      "Epoch 7/1000\n",
      "1948/1948 [==============================] - 0s 7us/sample - loss: 331875.5342 - mse: 331875.5625 - val_loss: 7428264.0000 - val_mse: 7428264.0000\n",
      "Epoch 8/1000\n",
      "1948/1948 [==============================] - 0s 9us/sample - loss: 330299.9643 - mse: 330299.9688 - val_loss: 7423484.0000 - val_mse: 7423484.0000\n",
      "Epoch 9/1000\n",
      "1948/1948 [==============================] - 0s 10us/sample - loss: 328735.3921 - mse: 328735.4062 - val_loss: 7417482.0000 - val_mse: 7417482.0000\n",
      "Epoch 10/1000\n",
      "1948/1948 [==============================] - 0s 12us/sample - loss: 326415.0459 - mse: 326415.0625 - val_loss: 7409971.5000 - val_mse: 7409971.5000\n",
      "Epoch 11/1000\n",
      "1948/1948 [==============================] - 0s 12us/sample - loss: 323834.5458 - mse: 323834.5312 - val_loss: 7400695.5000 - val_mse: 7400695.5000\n",
      "Epoch 12/1000\n",
      "1948/1948 [==============================] - 0s 12us/sample - loss: 320373.1349 - mse: 320373.1562 - val_loss: 7389156.5000 - val_mse: 7389156.5000\n",
      "Epoch 13/1000\n",
      "1948/1948 [==============================] - 0s 12us/sample - loss: 316412.2341 - mse: 316412.2500 - val_loss: 7374447.5000 - val_mse: 7374447.5000\n",
      "Epoch 14/1000\n",
      "1948/1948 [==============================] - 0s 12us/sample - loss: 312368.3490 - mse: 312368.3750 - val_loss: 7356160.5000 - val_mse: 7356160.5000\n",
      "Epoch 15/1000\n",
      "1948/1948 [==============================] - 0s 12us/sample - loss: 307545.8049 - mse: 307545.8125 - val_loss: 7335148.0000 - val_mse: 7335148.0000\n",
      "Epoch 16/1000\n",
      "1948/1948 [==============================] - 0s 12us/sample - loss: 301569.5309 - mse: 301569.5000 - val_loss: 7308936.5000 - val_mse: 7308936.5000\n",
      "Epoch 17/1000\n",
      "1948/1948 [==============================] - 0s 12us/sample - loss: 295644.8388 - mse: 295644.8438 - val_loss: 7278508.0000 - val_mse: 7278508.0000\n",
      "Epoch 18/1000\n",
      "1948/1948 [==============================] - 0s 12us/sample - loss: 288606.9296 - mse: 288606.9375 - val_loss: 7243898.5000 - val_mse: 7243898.5000\n",
      "Epoch 19/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 281919.8178 - mse: 281919.8125 - val_loss: 7205291.5000 - val_mse: 7205291.5000\n",
      "Epoch 20/1000\n",
      "1948/1948 [==============================] - 0s 12us/sample - loss: 276246.4724 - mse: 276246.4688 - val_loss: 7160691.0000 - val_mse: 7160691.0000\n",
      "Epoch 21/1000\n",
      "1948/1948 [==============================] - 0s 12us/sample - loss: 270946.5289 - mse: 270946.5312 - val_loss: 7111968.0000 - val_mse: 7111968.0000\n",
      "Epoch 22/1000\n",
      "1948/1948 [==============================] - 0s 12us/sample - loss: 266146.1503 - mse: 266146.1562 - val_loss: 7061217.5000 - val_mse: 7061217.5000\n",
      "Epoch 23/1000\n",
      "1948/1948 [==============================] - 0s 12us/sample - loss: 263309.4023 - mse: 263309.4062 - val_loss: 7007410.0000 - val_mse: 7007410.0000\n",
      "Epoch 24/1000\n",
      "1948/1948 [==============================] - 0s 12us/sample - loss: 260616.5015 - mse: 260616.5156 - val_loss: 6955558.0000 - val_mse: 6955558.0000\n",
      "Epoch 25/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 258229.6284 - mse: 258229.6250 - val_loss: 6906945.5000 - val_mse: 6906945.5000\n",
      "Epoch 26/1000\n",
      "1948/1948 [==============================] - 0s 12us/sample - loss: 257964.7540 - mse: 257964.7500 - val_loss: 6863966.0000 - val_mse: 6863966.0000\n",
      "Epoch 27/1000\n",
      "1948/1948 [==============================] - 0s 12us/sample - loss: 255928.1244 - mse: 255928.1094 - val_loss: 6824681.5000 - val_mse: 6824681.5000\n",
      "Epoch 28/1000\n",
      "1948/1948 [==============================] - 0s 12us/sample - loss: 254153.2240 - mse: 254153.2344 - val_loss: 6790459.5000 - val_mse: 6790459.5000\n",
      "Epoch 29/1000\n",
      "1948/1948 [==============================] - 0s 12us/sample - loss: 252326.4876 - mse: 252326.4844 - val_loss: 6761783.5000 - val_mse: 6761783.5000\n",
      "Epoch 30/1000\n",
      "1948/1948 [==============================] - 0s 12us/sample - loss: 247919.5790 - mse: 247919.5781 - val_loss: 6739469.5000 - val_mse: 6739469.5000\n",
      "Epoch 31/1000\n",
      "1948/1948 [==============================] - 0s 12us/sample - loss: 243125.2247 - mse: 243125.2188 - val_loss: 6716244.5000 - val_mse: 6716244.5000\n",
      "Epoch 32/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 245567.4248 - mse: 245567.4219 - val_loss: 6697617.5000 - val_mse: 6697617.5000\n",
      "Epoch 33/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 238109.7965 - mse: 238109.7969 - val_loss: 6678502.5000 - val_mse: 6678502.5000\n",
      "Epoch 34/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 236608.0178 - mse: 236608.0156 - val_loss: 6653330.5000 - val_mse: 6653330.5000\n",
      "Epoch 35/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 233151.4773 - mse: 233151.4688 - val_loss: 6622600.5000 - val_mse: 6622600.5000\n",
      "Epoch 36/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 237275.6012 - mse: 237275.5938 - val_loss: 6590201.0000 - val_mse: 6590201.0000\n",
      "Epoch 37/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 231677.8091 - mse: 231677.7969 - val_loss: 6550872.0000 - val_mse: 6550872.0000\n",
      "Epoch 38/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 226599.8633 - mse: 226599.8594 - val_loss: 6507073.0000 - val_mse: 6507073.0000\n",
      "Epoch 39/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 220824.5329 - mse: 220824.5469 - val_loss: 6458771.0000 - val_mse: 6458771.0000\n",
      "Epoch 40/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 221691.0220 - mse: 221691.0156 - val_loss: 6403276.5000 - val_mse: 6403276.5000\n",
      "Epoch 41/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 213568.3262 - mse: 213568.3281 - val_loss: 6342176.0000 - val_mse: 6342176.0000\n",
      "Epoch 42/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 206851.2428 - mse: 206851.2500 - val_loss: 6274897.0000 - val_mse: 6274897.0000\n",
      "Epoch 43/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 201221.3229 - mse: 201221.3281 - val_loss: 6203239.0000 - val_mse: 6203239.0000\n",
      "Epoch 44/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 194788.3030 - mse: 194788.2969 - val_loss: 6126590.0000 - val_mse: 6126590.0000\n",
      "Epoch 45/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 191816.4850 - mse: 191816.4688 - val_loss: 6047654.0000 - val_mse: 6047654.0000\n",
      "Epoch 46/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 185520.5235 - mse: 185520.5312 - val_loss: 5963207.0000 - val_mse: 5963207.0000\n",
      "Epoch 47/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 179957.1807 - mse: 179957.1875 - val_loss: 5872179.5000 - val_mse: 5872179.5000\n",
      "Epoch 48/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 168153.8642 - mse: 168153.8594 - val_loss: 5772822.0000 - val_mse: 5772822.0000\n",
      "Epoch 49/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 166665.0655 - mse: 166665.0625 - val_loss: 5667737.0000 - val_mse: 5667737.0000\n",
      "Epoch 50/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 153860.8887 - mse: 153860.8906 - val_loss: 5561821.0000 - val_mse: 5561821.0000\n",
      "Epoch 51/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 143837.4318 - mse: 143837.4375 - val_loss: 5453184.5000 - val_mse: 5453184.5000\n",
      "Epoch 52/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 148876.3834 - mse: 148876.3906 - val_loss: 5345878.5000 - val_mse: 5345878.5000\n",
      "Epoch 53/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 136625.0457 - mse: 136625.0469 - val_loss: 5232978.0000 - val_mse: 5232978.0000\n",
      "Epoch 54/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 132870.7201 - mse: 132870.7188 - val_loss: 5114956.0000 - val_mse: 5114956.0000\n",
      "Epoch 55/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 116327.9921 - mse: 116327.9922 - val_loss: 4998100.5000 - val_mse: 4998100.5000\n",
      "Epoch 56/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 129612.4446 - mse: 129612.4453 - val_loss: 4882994.5000 - val_mse: 4882994.5000\n",
      "Epoch 57/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 114784.5701 - mse: 114784.5781 - val_loss: 4768291.5000 - val_mse: 4768291.5000\n",
      "Epoch 58/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 109643.2142 - mse: 109643.2188 - val_loss: 4660754.5000 - val_mse: 4660754.5000\n",
      "Epoch 59/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 107234.2451 - mse: 107234.2500 - val_loss: 4552836.0000 - val_mse: 4552836.0000\n",
      "Epoch 60/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 94283.1016 - mse: 94283.1016 - val_loss: 4442731.0000 - val_mse: 4442731.0000\n",
      "Epoch 61/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 97519.1563 - mse: 97519.1641 - val_loss: 4341503.5000 - val_mse: 4341503.5000\n",
      "Epoch 62/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 84598.7173 - mse: 84598.7188 - val_loss: 4235241.5000 - val_mse: 4235241.5000\n",
      "Epoch 63/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 79805.3250 - mse: 79805.3203 - val_loss: 4137429.0000 - val_mse: 4137429.0000\n",
      "Epoch 64/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 74888.4551 - mse: 74888.4609 - val_loss: 4054513.7500 - val_mse: 4054513.7500\n",
      "Epoch 65/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 82840.3967 - mse: 82840.3906 - val_loss: 3979009.5000 - val_mse: 3979009.5000\n",
      "Epoch 66/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 80956.3148 - mse: 80956.3125 - val_loss: 3881007.2500 - val_mse: 3881007.2500\n",
      "Epoch 67/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 70453.2861 - mse: 70453.2891 - val_loss: 3797162.2500 - val_mse: 3797162.2500\n",
      "Epoch 68/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 76559.9590 - mse: 76559.9609 - val_loss: 3715134.0000 - val_mse: 3715134.0000\n",
      "Epoch 69/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 66968.6116 - mse: 66968.6094 - val_loss: 3628954.2500 - val_mse: 3628954.2500\n",
      "Epoch 70/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 64225.2953 - mse: 64225.2969 - val_loss: 3526042.7500 - val_mse: 3526042.7500\n",
      "Epoch 71/1000\n",
      "1948/1948 [==============================] - 0s 16us/sample - loss: 62757.5725 - mse: 62757.5742 - val_loss: 3452931.2500 - val_mse: 3452931.2500\n",
      "Epoch 72/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 59552.4738 - mse: 59552.4766 - val_loss: 3385957.7500 - val_mse: 3385957.7500\n",
      "Epoch 73/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 57063.3609 - mse: 57063.3594 - val_loss: 3316063.7500 - val_mse: 3316063.7500\n",
      "Epoch 74/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 64463.1947 - mse: 64463.1953 - val_loss: 3261241.2500 - val_mse: 3261241.2500\n",
      "Epoch 75/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 63184.3226 - mse: 63184.3203 - val_loss: 3223499.0000 - val_mse: 3223499.0000\n",
      "Epoch 76/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 75023.8230 - mse: 75023.8203 - val_loss: 3214126.0000 - val_mse: 3214126.0000\n",
      "Epoch 77/1000\n",
      "1948/1948 [==============================] - 0s 16us/sample - loss: 56923.1900 - mse: 56923.1914 - val_loss: 3256110.5000 - val_mse: 3256110.5000\n",
      "Epoch 78/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 60607.7275 - mse: 60607.7305 - val_loss: 3198545.5000 - val_mse: 3198545.5000\n",
      "Epoch 79/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 46908.8686 - mse: 46908.8711 - val_loss: 3108222.5000 - val_mse: 3108222.5000\n",
      "Epoch 80/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 54937.6210 - mse: 54937.6172 - val_loss: 3046443.0000 - val_mse: 3046443.0000\n",
      "Epoch 81/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 49472.2877 - mse: 49472.2891 - val_loss: 3026038.7500 - val_mse: 3026038.7500\n",
      "Epoch 82/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 48775.9384 - mse: 48775.9375 - val_loss: 2955686.0000 - val_mse: 2955686.0000\n",
      "Epoch 83/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 46420.2916 - mse: 46420.2930 - val_loss: 2929232.0000 - val_mse: 2929232.0000\n",
      "Epoch 84/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 45130.6620 - mse: 45130.6602 - val_loss: 2897043.0000 - val_mse: 2897043.0000\n",
      "Epoch 85/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 43772.5922 - mse: 43772.5898 - val_loss: 2857719.2500 - val_mse: 2857719.2500\n",
      "Epoch 86/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 46114.9577 - mse: 46114.9570 - val_loss: 2823297.5000 - val_mse: 2823297.5000\n",
      "Epoch 87/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 44750.8765 - mse: 44750.8750 - val_loss: 2788402.7500 - val_mse: 2788402.7500\n",
      "Epoch 88/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 35976.2144 - mse: 35976.2148 - val_loss: 2765140.0000 - val_mse: 2765140.0000\n",
      "Epoch 89/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 50108.3778 - mse: 50108.3789 - val_loss: 2728967.2500 - val_mse: 2728967.2500\n",
      "Epoch 90/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 43112.7028 - mse: 43112.7031 - val_loss: 2665202.5000 - val_mse: 2665202.5000\n",
      "Epoch 91/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 37255.1217 - mse: 37255.1211 - val_loss: 2608704.2500 - val_mse: 2608704.2500\n",
      "Epoch 92/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 39349.1125 - mse: 39349.1133 - val_loss: 2577262.2500 - val_mse: 2577262.2500\n",
      "Epoch 93/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 53210.6605 - mse: 53210.6602 - val_loss: 2558513.5000 - val_mse: 2558513.5000\n",
      "Epoch 94/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 38052.1483 - mse: 38052.1484 - val_loss: 2551347.0000 - val_mse: 2551347.0000\n",
      "Epoch 95/1000\n",
      "1948/1948 [==============================] - 0s 18us/sample - loss: 35147.6461 - mse: 35147.6484 - val_loss: 2568865.0000 - val_mse: 2568865.0000\n",
      "Epoch 96/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 31766.4056 - mse: 31766.4062 - val_loss: 2582339.2500 - val_mse: 2582339.2500\n",
      "Epoch 97/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 41486.0380 - mse: 41486.0352 - val_loss: 2610479.0000 - val_mse: 2610479.0000\n",
      "Epoch 98/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 41945.0700 - mse: 41945.0664 - val_loss: 2586841.2500 - val_mse: 2586841.2500\n",
      "Epoch 99/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 37727.4878 - mse: 37727.4883 - val_loss: 2527477.5000 - val_mse: 2527477.5000\n",
      "Epoch 100/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 34315.1804 - mse: 34315.1797 - val_loss: 2477389.0000 - val_mse: 2477389.0000\n",
      "Epoch 101/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 31514.5181 - mse: 31514.5176 - val_loss: 2468788.5000 - val_mse: 2468788.5000\n",
      "Epoch 102/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 35009.8674 - mse: 35009.8672 - val_loss: 2462650.7500 - val_mse: 2462650.7500\n",
      "Epoch 103/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 30774.5871 - mse: 30774.5879 - val_loss: 2446697.0000 - val_mse: 2446697.0000\n",
      "Epoch 104/1000\n",
      "1948/1948 [==============================] - 0s 18us/sample - loss: 33905.6024 - mse: 33905.6016 - val_loss: 2421396.7500 - val_mse: 2421396.7500\n",
      "Epoch 105/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 36672.9634 - mse: 36672.9609 - val_loss: 2411845.2500 - val_mse: 2411845.2500\n",
      "Epoch 106/1000\n",
      "1948/1948 [==============================] - 0s 18us/sample - loss: 45125.0904 - mse: 45125.0938 - val_loss: 2383201.7500 - val_mse: 2383201.7500\n",
      "Epoch 107/1000\n",
      "1948/1948 [==============================] - 0s 20us/sample - loss: 36081.0609 - mse: 36081.0586 - val_loss: 2366855.7500 - val_mse: 2366855.7500\n",
      "Epoch 108/1000\n",
      "1948/1948 [==============================] - 0s 20us/sample - loss: 29894.2276 - mse: 29894.2285 - val_loss: 2347873.0000 - val_mse: 2347873.0000\n",
      "Epoch 109/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 44849.2645 - mse: 44849.2656 - val_loss: 2328838.5000 - val_mse: 2328838.5000\n",
      "Epoch 110/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 36694.8528 - mse: 36694.8555 - val_loss: 2312851.2500 - val_mse: 2312851.2500\n",
      "Epoch 111/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 33425.8424 - mse: 33425.8438 - val_loss: 2303122.2500 - val_mse: 2303122.2500\n",
      "Epoch 112/1000\n",
      "1948/1948 [==============================] - 0s 20us/sample - loss: 38923.5826 - mse: 38923.5820 - val_loss: 2301654.5000 - val_mse: 2301654.5000\n",
      "Epoch 113/1000\n",
      "1948/1948 [==============================] - 0s 18us/sample - loss: 33797.6227 - mse: 33797.6211 - val_loss: 2273195.7500 - val_mse: 2273195.7500\n",
      "Epoch 114/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 32685.9146 - mse: 32685.9141 - val_loss: 2233076.2500 - val_mse: 2233076.2500\n",
      "Epoch 115/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 33391.4882 - mse: 33391.4883 - val_loss: 2220660.0000 - val_mse: 2220660.0000\n",
      "Epoch 116/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 27832.7971 - mse: 27832.7969 - val_loss: 2214023.7500 - val_mse: 2214023.7500\n",
      "Epoch 117/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 24841.9547 - mse: 24841.9551 - val_loss: 2203555.0000 - val_mse: 2203555.0000\n",
      "Epoch 118/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 26613.3272 - mse: 26613.3262 - val_loss: 2185238.5000 - val_mse: 2185238.5000\n",
      "Epoch 119/1000\n",
      "1948/1948 [==============================] - 0s 18us/sample - loss: 36795.5272 - mse: 36795.5273 - val_loss: 2179129.0000 - val_mse: 2179129.0000\n",
      "Epoch 120/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 36967.8177 - mse: 36967.8203 - val_loss: 2176140.7500 - val_mse: 2176140.7500\n",
      "Epoch 121/1000\n",
      "1948/1948 [==============================] - 0s 20us/sample - loss: 44416.5556 - mse: 44416.5586 - val_loss: 2154104.0000 - val_mse: 2154104.0000\n",
      "Epoch 122/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 22700.9057 - mse: 22700.9062 - val_loss: 2119400.5000 - val_mse: 2119400.5000\n",
      "Epoch 123/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 29444.1362 - mse: 29444.1348 - val_loss: 2094254.1250 - val_mse: 2094254.1250\n",
      "Epoch 124/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 41561.1641 - mse: 41561.1680 - val_loss: 2064092.7500 - val_mse: 2064092.7500\n",
      "Epoch 125/1000\n",
      "1948/1948 [==============================] - 0s 20us/sample - loss: 31216.3684 - mse: 31216.3691 - val_loss: 2050334.2500 - val_mse: 2050334.2500\n",
      "Epoch 126/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 35561.8498 - mse: 35561.8477 - val_loss: 2094805.6250 - val_mse: 2094805.6250\n",
      "Epoch 127/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 31305.2657 - mse: 31305.2656 - val_loss: 2169800.2500 - val_mse: 2169800.2500\n",
      "Epoch 128/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 32774.4456 - mse: 32774.4453 - val_loss: 2172105.2500 - val_mse: 2172105.2500\n",
      "Epoch 129/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 37207.1984 - mse: 37207.1992 - val_loss: 2206072.7500 - val_mse: 2206072.7500\n",
      "Epoch 130/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 32839.4773 - mse: 32839.4766 - val_loss: 2191476.5000 - val_mse: 2191476.5000\n",
      "Epoch 131/1000\n",
      "1948/1948 [==============================] - 0s 18us/sample - loss: 31280.2157 - mse: 31280.2148 - val_loss: 2114264.0000 - val_mse: 2114264.0000\n",
      "Epoch 132/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 30887.4074 - mse: 30887.4082 - val_loss: 2047484.8750 - val_mse: 2047484.8750\n",
      "Epoch 133/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 27804.7356 - mse: 27804.7344 - val_loss: 2053340.6250 - val_mse: 2053340.6250\n",
      "Epoch 134/1000\n",
      "1948/1948 [==============================] - 0s 20us/sample - loss: 31408.2266 - mse: 31408.2266 - val_loss: 2082842.7500 - val_mse: 2082842.7500\n",
      "Epoch 135/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 42705.8187 - mse: 42705.8203 - val_loss: 2110151.7500 - val_mse: 2110151.7500\n",
      "Epoch 136/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 26730.0895 - mse: 26730.0898 - val_loss: 2117119.5000 - val_mse: 2117119.5000\n",
      "Epoch 137/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 33708.0764 - mse: 33708.0742 - val_loss: 2106382.7500 - val_mse: 2106382.7500\n",
      "Epoch 138/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 27734.0657 - mse: 27734.0664 - val_loss: 2078501.2500 - val_mse: 2078501.2500\n",
      "Epoch 139/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 29041.0603 - mse: 29041.0586 - val_loss: 2066679.1250 - val_mse: 2066679.1250\n",
      "Epoch 140/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 30617.4519 - mse: 30617.4512 - val_loss: 2041710.6250 - val_mse: 2041710.6250\n",
      "Epoch 141/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 29410.3406 - mse: 29410.3418 - val_loss: 2025432.7500 - val_mse: 2025432.7500\n",
      "Epoch 142/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 34464.9036 - mse: 34464.9023 - val_loss: 2020243.3750 - val_mse: 2020243.3750\n",
      "Epoch 143/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 27563.3441 - mse: 27563.3438 - val_loss: 2023913.5000 - val_mse: 2023913.5000\n",
      "Epoch 144/1000\n",
      "1948/1948 [==============================] - 0s 18us/sample - loss: 33012.4626 - mse: 33012.4648 - val_loss: 2022854.6250 - val_mse: 2022854.6250\n",
      "Epoch 145/1000\n",
      "1948/1948 [==============================] - 0s 18us/sample - loss: 30545.6102 - mse: 30545.6094 - val_loss: 2015835.3750 - val_mse: 2015835.3750\n",
      "Epoch 146/1000\n",
      "1948/1948 [==============================] - 0s 20us/sample - loss: 29980.0430 - mse: 29980.0410 - val_loss: 1998424.7500 - val_mse: 1998424.7500\n",
      "Epoch 147/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 29636.9031 - mse: 29636.9043 - val_loss: 1978027.5000 - val_mse: 1978027.5000\n",
      "Epoch 148/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 37478.2042 - mse: 37478.2070 - val_loss: 1967294.3750 - val_mse: 1967294.3750\n",
      "Epoch 149/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 24794.2746 - mse: 24794.2754 - val_loss: 1970172.0000 - val_mse: 1970172.0000\n",
      "Epoch 150/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 21375.4631 - mse: 21375.4629 - val_loss: 1977285.5000 - val_mse: 1977285.5000\n",
      "Epoch 151/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 32545.1211 - mse: 32545.1211 - val_loss: 2011187.0000 - val_mse: 2011187.0000\n",
      "Epoch 152/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 36876.3265 - mse: 36876.3281 - val_loss: 2054045.8750 - val_mse: 2054045.8750\n",
      "Epoch 153/1000\n",
      "1948/1948 [==============================] - 0s 18us/sample - loss: 28939.4365 - mse: 28939.4375 - val_loss: 2120273.0000 - val_mse: 2120273.0000\n",
      "Epoch 154/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 32792.5296 - mse: 32792.5312 - val_loss: 2131007.7500 - val_mse: 2131007.7500\n",
      "Epoch 155/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 27231.4358 - mse: 27231.4355 - val_loss: 2103608.0000 - val_mse: 2103608.0000\n",
      "Epoch 156/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 27310.3021 - mse: 27310.3027 - val_loss: 2068646.0000 - val_mse: 2068646.0000\n",
      "Epoch 157/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 31080.1205 - mse: 31080.1191 - val_loss: 2019857.8750 - val_mse: 2019857.8750\n",
      "Epoch 158/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 36104.3059 - mse: 36104.3047 - val_loss: 2012874.6250 - val_mse: 2012874.6250\n",
      "Epoch 159/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 35999.0527 - mse: 35999.0547 - val_loss: 2004107.0000 - val_mse: 2004107.0000\n",
      "Epoch 160/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 24861.1736 - mse: 24861.1738 - val_loss: 2042064.5000 - val_mse: 2042064.5000\n",
      "Epoch 161/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 27876.8959 - mse: 27876.8945 - val_loss: 2040626.0000 - val_mse: 2040626.0000\n",
      "Epoch 162/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 36355.0267 - mse: 36355.0273 - val_loss: 2029398.0000 - val_mse: 2029398.0000\n",
      "Epoch 163/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 31330.1437 - mse: 31330.1445 - val_loss: 2015973.6250 - val_mse: 2015973.6250\n",
      "Epoch 164/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 33000.0810 - mse: 33000.0820 - val_loss: 2018800.2500 - val_mse: 2018800.2500\n",
      "Epoch 165/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 25318.5661 - mse: 25318.5664 - val_loss: 2028474.7500 - val_mse: 2028474.7500\n",
      "Epoch 166/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 29599.6161 - mse: 29599.6152 - val_loss: 2040228.1250 - val_mse: 2040228.1250\n",
      "Epoch 167/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 25887.3316 - mse: 25887.3301 - val_loss: 2068361.1250 - val_mse: 2068361.1250\n",
      "Epoch 168/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 30480.5671 - mse: 30480.5664 - val_loss: 2082368.6250 - val_mse: 2082368.6250\n",
      "Epoch 169/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 28814.6288 - mse: 28814.6289 - val_loss: 2094205.8750 - val_mse: 2094205.8750\n",
      "Epoch 170/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 30094.6981 - mse: 30094.6973 - val_loss: 2095927.2500 - val_mse: 2095927.2500\n",
      "Epoch 171/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 27355.4326 - mse: 27355.4336 - val_loss: 2093362.0000 - val_mse: 2093362.0000\n",
      "Epoch 172/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 26852.0078 - mse: 26852.0078 - val_loss: 2100443.5000 - val_mse: 2100443.5000\n",
      "Epoch 173/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 28349.8155 - mse: 28349.8145 - val_loss: 2065022.8750 - val_mse: 2065022.8750\n",
      "Epoch 174/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 23022.6865 - mse: 23022.6855 - val_loss: 2014484.0000 - val_mse: 2014484.0000\n",
      "Epoch 175/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 24750.4811 - mse: 24750.4805 - val_loss: 1998786.7500 - val_mse: 1998786.7500\n",
      "Epoch 176/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 22676.7580 - mse: 22676.7578 - val_loss: 2033587.1250 - val_mse: 2033587.1250\n",
      "Epoch 177/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 28851.4174 - mse: 28851.4160 - val_loss: 2059025.2500 - val_mse: 2059025.2500\n",
      "Epoch 178/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 26198.9785 - mse: 26198.9766 - val_loss: 2075957.5000 - val_mse: 2075957.5000\n",
      "Epoch 179/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 33441.5747 - mse: 33441.5742 - val_loss: 2093119.0000 - val_mse: 2093119.0000\n",
      "Epoch 180/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 26162.0599 - mse: 26162.0586 - val_loss: 2077974.7500 - val_mse: 2077974.7500\n",
      "Epoch 181/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 35541.1574 - mse: 35541.1562 - val_loss: 2044845.1250 - val_mse: 2044845.1250\n",
      "Epoch 182/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 30466.7104 - mse: 30466.7109 - val_loss: 1993654.7500 - val_mse: 1993654.7500\n",
      "Epoch 183/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 27868.9166 - mse: 27868.9160 - val_loss: 1920100.8750 - val_mse: 1920100.8750\n",
      "Epoch 184/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 26287.3211 - mse: 26287.3223 - val_loss: 1889052.8750 - val_mse: 1889052.8750\n",
      "Epoch 185/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 29389.1136 - mse: 29389.1133 - val_loss: 1875960.7500 - val_mse: 1875960.7500\n",
      "Epoch 186/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 26718.0513 - mse: 26718.0508 - val_loss: 1916964.8750 - val_mse: 1916964.8750\n",
      "Epoch 187/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 35119.2832 - mse: 35119.2812 - val_loss: 1998035.6250 - val_mse: 1998035.6250\n",
      "Epoch 188/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 24669.6926 - mse: 24669.6914 - val_loss: 2061189.5000 - val_mse: 2061189.5000\n",
      "Epoch 189/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 26448.9084 - mse: 26448.9082 - val_loss: 2062681.7500 - val_mse: 2062681.7500\n",
      "Epoch 190/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 24756.8975 - mse: 24756.8965 - val_loss: 2020660.3750 - val_mse: 2020660.3750\n",
      "Epoch 191/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 32405.1759 - mse: 32405.1738 - val_loss: 1990618.1250 - val_mse: 1990618.1250\n",
      "Epoch 192/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 28774.7656 - mse: 28774.7656 - val_loss: 1958269.5000 - val_mse: 1958269.5000\n",
      "Epoch 193/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 28757.1633 - mse: 28757.1621 - val_loss: 1916630.5000 - val_mse: 1916630.5000\n",
      "Epoch 194/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 36628.9131 - mse: 36628.9102 - val_loss: 1889376.0000 - val_mse: 1889376.0000\n",
      "Epoch 195/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 27003.7020 - mse: 27003.7031 - val_loss: 1923856.1250 - val_mse: 1923856.1250\n",
      "Epoch 196/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 26288.8674 - mse: 26288.8672 - val_loss: 1936100.1250 - val_mse: 1936100.1250\n",
      "Epoch 197/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23557.1127 - mse: 23557.1133 - val_loss: 1954318.8750 - val_mse: 1954318.8750\n",
      "Epoch 198/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 24761.4427 - mse: 24761.4414 - val_loss: 1946711.6250 - val_mse: 1946711.6250\n",
      "Epoch 199/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 32048.9982 - mse: 32048.9980 - val_loss: 1909247.0000 - val_mse: 1909247.0000\n",
      "Epoch 200/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 24353.4628 - mse: 24353.4629 - val_loss: 1889033.5000 - val_mse: 1889033.5000\n",
      "Epoch 201/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 26928.3646 - mse: 26928.3652 - val_loss: 1877662.6250 - val_mse: 1877662.6250\n",
      "Epoch 202/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 29158.7861 - mse: 29158.7871 - val_loss: 1916412.8750 - val_mse: 1916412.8750\n",
      "Epoch 203/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 27147.7647 - mse: 27147.7656 - val_loss: 1984991.1250 - val_mse: 1984991.1250\n",
      "Epoch 204/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 31319.7564 - mse: 31319.7578 - val_loss: 2010940.7500 - val_mse: 2010940.7500\n",
      "Epoch 205/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 30041.4422 - mse: 30041.4414 - val_loss: 2000044.5000 - val_mse: 2000044.5000\n",
      "Epoch 206/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 26392.0088 - mse: 26392.0078 - val_loss: 1982355.3750 - val_mse: 1982355.3750\n",
      "Epoch 207/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 28105.2524 - mse: 28105.2520 - val_loss: 1954634.1250 - val_mse: 1954634.1250\n",
      "Epoch 208/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23014.9819 - mse: 23014.9824 - val_loss: 1911024.2500 - val_mse: 1911024.2500\n",
      "Epoch 209/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 27052.7141 - mse: 27052.7148 - val_loss: 1889586.3750 - val_mse: 1889586.3750\n",
      "Epoch 210/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 25596.5619 - mse: 25596.5625 - val_loss: 1882902.0000 - val_mse: 1882902.0000\n",
      "Epoch 211/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 24151.6897 - mse: 24151.6895 - val_loss: 1861008.3750 - val_mse: 1861008.3750\n",
      "Epoch 212/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23895.1784 - mse: 23895.1777 - val_loss: 1828384.2500 - val_mse: 1828384.2500\n",
      "Epoch 213/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 33453.9070 - mse: 33453.9062 - val_loss: 1864396.1250 - val_mse: 1864396.1250\n",
      "Epoch 214/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 30606.0789 - mse: 30606.0781 - val_loss: 1896000.7500 - val_mse: 1896000.7500\n",
      "Epoch 215/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 26518.9293 - mse: 26518.9277 - val_loss: 1894707.8750 - val_mse: 1894707.8750\n",
      "Epoch 216/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 28865.8871 - mse: 28865.8867 - val_loss: 1898128.3750 - val_mse: 1898128.3750\n",
      "Epoch 217/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 25341.3847 - mse: 25341.3848 - val_loss: 1894041.7500 - val_mse: 1894041.7500\n",
      "Epoch 218/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 22827.4453 - mse: 22827.4453 - val_loss: 1909773.1250 - val_mse: 1909773.1250\n",
      "Epoch 219/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23838.3705 - mse: 23838.3691 - val_loss: 1901554.1250 - val_mse: 1901554.1250\n",
      "Epoch 220/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 27637.5001 - mse: 27637.4980 - val_loss: 1889706.6250 - val_mse: 1889706.6250\n",
      "Epoch 221/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 28896.9455 - mse: 28896.9453 - val_loss: 1894092.3750 - val_mse: 1894092.3750\n",
      "Epoch 222/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 26659.4823 - mse: 26659.4824 - val_loss: 1901532.7500 - val_mse: 1901532.7500\n",
      "Epoch 223/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 24535.0936 - mse: 24535.0918 - val_loss: 1905950.2500 - val_mse: 1905950.2500\n",
      "Epoch 224/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 22727.8892 - mse: 22727.8887 - val_loss: 1919965.5000 - val_mse: 1919965.5000\n",
      "Epoch 225/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 28935.2227 - mse: 28935.2246 - val_loss: 1938750.3750 - val_mse: 1938750.3750\n",
      "Epoch 226/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 31624.3181 - mse: 31624.3184 - val_loss: 1969467.3750 - val_mse: 1969467.3750\n",
      "Epoch 227/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 25812.5329 - mse: 25812.5332 - val_loss: 2011482.2500 - val_mse: 2011482.2500\n",
      "Epoch 228/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 34361.9775 - mse: 34361.9766 - val_loss: 2023236.1250 - val_mse: 2023236.1250\n",
      "Epoch 229/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 23837.9751 - mse: 23837.9746 - val_loss: 1971574.3750 - val_mse: 1971574.3750\n",
      "Epoch 230/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 21904.9672 - mse: 21904.9668 - val_loss: 1922180.8750 - val_mse: 1922180.8750\n",
      "Epoch 231/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 27411.1325 - mse: 27411.1328 - val_loss: 1898382.1250 - val_mse: 1898382.1250\n",
      "Epoch 232/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23388.7497 - mse: 23388.7500 - val_loss: 1924497.2500 - val_mse: 1924497.2500\n",
      "Epoch 233/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 24243.3051 - mse: 24243.3066 - val_loss: 1934131.7500 - val_mse: 1934131.7500\n",
      "Epoch 234/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 22025.6428 - mse: 22025.6426 - val_loss: 1924068.0000 - val_mse: 1924068.0000\n",
      "Epoch 235/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 29876.1008 - mse: 29876.1016 - val_loss: 1912615.2500 - val_mse: 1912615.2500\n",
      "Epoch 236/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 21534.9767 - mse: 21534.9766 - val_loss: 1913583.5000 - val_mse: 1913583.5000\n",
      "Epoch 237/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 20147.2421 - mse: 20147.2422 - val_loss: 1915125.2500 - val_mse: 1915125.2500\n",
      "Epoch 238/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 23804.9854 - mse: 23804.9863 - val_loss: 1910948.3750 - val_mse: 1910948.3750\n",
      "Epoch 239/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 20408.0769 - mse: 20408.0781 - val_loss: 1900038.2500 - val_mse: 1900038.2500\n",
      "Epoch 240/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 29479.3580 - mse: 29479.3594 - val_loss: 1896571.6250 - val_mse: 1896571.6250\n",
      "Epoch 241/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 27556.5122 - mse: 27556.5137 - val_loss: 1905965.0000 - val_mse: 1905965.0000\n",
      "Epoch 242/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 23512.6033 - mse: 23512.6035 - val_loss: 1911275.3750 - val_mse: 1911275.3750\n",
      "Epoch 243/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 23830.4129 - mse: 23830.4121 - val_loss: 1980169.0000 - val_mse: 1980169.0000\n",
      "Epoch 244/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 33338.2125 - mse: 33338.2148 - val_loss: 2051305.8750 - val_mse: 2051305.8750\n",
      "Epoch 245/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 29714.5158 - mse: 29714.5156 - val_loss: 2077108.6250 - val_mse: 2077108.6250\n",
      "Epoch 246/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 24286.6854 - mse: 24286.6855 - val_loss: 2078429.7500 - val_mse: 2078429.7500\n",
      "Epoch 247/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 27279.9684 - mse: 27279.9668 - val_loss: 2052201.8750 - val_mse: 2052201.8750\n",
      "Epoch 248/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 26500.3296 - mse: 26500.3281 - val_loss: 2006792.2500 - val_mse: 2006792.2500\n",
      "Epoch 249/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 19175.4749 - mse: 19175.4746 - val_loss: 1950758.3750 - val_mse: 1950758.3750\n",
      "Epoch 250/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 27149.7646 - mse: 27149.7656 - val_loss: 1951428.7500 - val_mse: 1951428.7500\n",
      "Epoch 251/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 19324.9384 - mse: 19324.9375 - val_loss: 1958760.5000 - val_mse: 1958760.5000\n",
      "Epoch 252/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 27087.4408 - mse: 27087.4414 - val_loss: 1961387.3750 - val_mse: 1961387.3750\n",
      "Epoch 253/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 21598.6631 - mse: 21598.6641 - val_loss: 1948518.7500 - val_mse: 1948518.7500\n",
      "Epoch 254/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 21570.1019 - mse: 21570.1035 - val_loss: 1906086.5000 - val_mse: 1906086.5000\n",
      "Epoch 255/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23085.3472 - mse: 23085.3477 - val_loss: 1900003.2500 - val_mse: 1900003.2500\n",
      "Epoch 256/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 28480.2177 - mse: 28480.2168 - val_loss: 1918129.8750 - val_mse: 1918129.8750\n",
      "Epoch 257/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 26140.8914 - mse: 26140.8906 - val_loss: 1957825.7500 - val_mse: 1957825.7500\n",
      "Epoch 258/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 26953.2004 - mse: 26953.1992 - val_loss: 1948620.0000 - val_mse: 1948620.0000\n",
      "Epoch 259/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23129.3820 - mse: 23129.3828 - val_loss: 1886816.8750 - val_mse: 1886816.8750\n",
      "Epoch 260/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 27294.1836 - mse: 27294.1855 - val_loss: 1864044.5000 - val_mse: 1864044.5000\n",
      "Epoch 261/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 23389.1677 - mse: 23389.1660 - val_loss: 1847802.1250 - val_mse: 1847802.1250\n",
      "Epoch 262/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 32891.6718 - mse: 32891.6719 - val_loss: 1868738.2500 - val_mse: 1868738.2500\n",
      "Epoch 263/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 25481.8059 - mse: 25481.8066 - val_loss: 1916226.5000 - val_mse: 1916226.5000\n",
      "Epoch 264/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 29155.8140 - mse: 29155.8145 - val_loss: 1919490.5000 - val_mse: 1919490.5000\n",
      "Epoch 265/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 20651.4438 - mse: 20651.4434 - val_loss: 1929440.6250 - val_mse: 1929440.6250\n",
      "Epoch 266/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 30616.3768 - mse: 30616.3770 - val_loss: 1935081.7500 - val_mse: 1935081.7500\n",
      "Epoch 267/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 27118.6270 - mse: 27118.6289 - val_loss: 1936028.3750 - val_mse: 1936028.3750\n",
      "Epoch 268/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 25569.4776 - mse: 25569.4785 - val_loss: 1930320.3750 - val_mse: 1930320.3750\n",
      "Epoch 269/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 19047.0561 - mse: 19047.0547 - val_loss: 1919694.5000 - val_mse: 1919694.5000\n",
      "Epoch 270/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 23166.0082 - mse: 23166.0078 - val_loss: 1872383.0000 - val_mse: 1872383.0000\n",
      "Epoch 271/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 24188.8843 - mse: 24188.8828 - val_loss: 1832387.8750 - val_mse: 1832387.8750\n",
      "Epoch 272/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 20944.0688 - mse: 20944.0703 - val_loss: 1813922.1250 - val_mse: 1813922.1250\n",
      "Epoch 273/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 24942.8324 - mse: 24942.8340 - val_loss: 1819753.7500 - val_mse: 1819753.7500\n",
      "Epoch 274/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 20240.3391 - mse: 20240.3379 - val_loss: 1829213.3750 - val_mse: 1829213.3750\n",
      "Epoch 275/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 25101.0162 - mse: 25101.0156 - val_loss: 1836100.6250 - val_mse: 1836100.6250\n",
      "Epoch 276/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 19565.9600 - mse: 19565.9590 - val_loss: 1839412.2500 - val_mse: 1839412.2500\n",
      "Epoch 277/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 25051.0193 - mse: 25051.0176 - val_loss: 1830470.7500 - val_mse: 1830470.7500\n",
      "Epoch 278/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23434.0790 - mse: 23434.0781 - val_loss: 1828133.1250 - val_mse: 1828133.1250\n",
      "Epoch 279/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 25715.7011 - mse: 25715.7012 - val_loss: 1843335.1250 - val_mse: 1843335.1250\n",
      "Epoch 280/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 23954.7555 - mse: 23954.7559 - val_loss: 1851294.6250 - val_mse: 1851294.6250\n",
      "Epoch 281/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 21299.6005 - mse: 21299.6016 - val_loss: 1847561.6250 - val_mse: 1847561.6250\n",
      "Epoch 282/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 27075.8878 - mse: 27075.8887 - val_loss: 1845342.1250 - val_mse: 1845342.1250\n",
      "Epoch 283/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 26964.6431 - mse: 26964.6426 - val_loss: 1828956.8750 - val_mse: 1828956.8750\n",
      "Epoch 284/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 24307.0589 - mse: 24307.0586 - val_loss: 1822497.5000 - val_mse: 1822497.5000\n",
      "Epoch 285/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 24844.0530 - mse: 24844.0527 - val_loss: 1828581.8750 - val_mse: 1828581.8750\n",
      "Epoch 286/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 25376.4432 - mse: 25376.4434 - val_loss: 1836255.2500 - val_mse: 1836255.2500\n",
      "Epoch 287/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 24153.3357 - mse: 24153.3340 - val_loss: 1821545.5000 - val_mse: 1821545.5000\n",
      "Epoch 288/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 25206.7464 - mse: 25206.7480 - val_loss: 1800087.1250 - val_mse: 1800087.1250\n",
      "Epoch 289/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 20860.6347 - mse: 20860.6348 - val_loss: 1801454.8750 - val_mse: 1801454.8750\n",
      "Epoch 290/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 21747.4021 - mse: 21747.4023 - val_loss: 1798319.0000 - val_mse: 1798319.0000\n",
      "Epoch 291/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23666.0217 - mse: 23666.0215 - val_loss: 1778701.2500 - val_mse: 1778701.2500\n",
      "Epoch 292/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 25989.4852 - mse: 25989.4863 - val_loss: 1725376.7500 - val_mse: 1725376.7500\n",
      "Epoch 293/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 24799.1880 - mse: 24799.1875 - val_loss: 1726020.8750 - val_mse: 1726020.8750\n",
      "Epoch 294/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 30161.9679 - mse: 30161.9668 - val_loss: 1778577.7500 - val_mse: 1778577.7500\n",
      "Epoch 295/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 21474.7235 - mse: 21474.7227 - val_loss: 1852140.8750 - val_mse: 1852140.8750\n",
      "Epoch 296/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 36692.1208 - mse: 36692.1250 - val_loss: 1897374.8750 - val_mse: 1897374.8750\n",
      "Epoch 297/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 30956.9478 - mse: 30956.9492 - val_loss: 1916605.8750 - val_mse: 1916605.8750\n",
      "Epoch 298/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 28166.1621 - mse: 28166.1621 - val_loss: 1885412.8750 - val_mse: 1885412.8750\n",
      "Epoch 299/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23603.8027 - mse: 23603.8027 - val_loss: 1836670.6250 - val_mse: 1836670.6250\n",
      "Epoch 300/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 26909.6820 - mse: 26909.6816 - val_loss: 1794889.5000 - val_mse: 1794889.5000\n",
      "Epoch 301/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23025.4773 - mse: 23025.4785 - val_loss: 1759118.0000 - val_mse: 1759118.0000\n",
      "Epoch 302/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 25484.4359 - mse: 25484.4355 - val_loss: 1727372.2500 - val_mse: 1727372.2500\n",
      "Epoch 303/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 25056.5223 - mse: 25056.5215 - val_loss: 1726218.0000 - val_mse: 1726218.0000\n",
      "Epoch 304/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 24795.2868 - mse: 24795.2852 - val_loss: 1752200.7500 - val_mse: 1752200.7500\n",
      "Epoch 305/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 31608.0300 - mse: 31608.0312 - val_loss: 1789655.2500 - val_mse: 1789655.2500\n",
      "Epoch 306/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 25091.6671 - mse: 25091.6680 - val_loss: 1811707.2500 - val_mse: 1811707.2500\n",
      "Epoch 307/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 24355.4477 - mse: 24355.4473 - val_loss: 1832928.7500 - val_mse: 1832928.7500\n",
      "Epoch 308/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 25067.4161 - mse: 25067.4160 - val_loss: 1863185.2500 - val_mse: 1863185.2500\n",
      "Epoch 309/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 30916.5781 - mse: 30916.5781 - val_loss: 1848210.1250 - val_mse: 1848210.1250\n",
      "Epoch 310/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 26261.0448 - mse: 26261.0449 - val_loss: 1832456.5000 - val_mse: 1832456.5000\n",
      "Epoch 311/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 20830.2369 - mse: 20830.2383 - val_loss: 1799405.3750 - val_mse: 1799405.3750\n",
      "Epoch 312/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23010.7781 - mse: 23010.7773 - val_loss: 1769446.7500 - val_mse: 1769446.7500\n",
      "Epoch 313/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 27463.4154 - mse: 27463.4160 - val_loss: 1767700.6250 - val_mse: 1767700.6250\n",
      "Epoch 314/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 25560.9458 - mse: 25560.9453 - val_loss: 1793069.3750 - val_mse: 1793069.3750\n",
      "Epoch 315/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 24350.1385 - mse: 24350.1387 - val_loss: 1815186.3750 - val_mse: 1815186.3750\n",
      "Epoch 316/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23280.1482 - mse: 23280.1484 - val_loss: 1826345.7500 - val_mse: 1826345.7500\n",
      "Epoch 317/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 21934.8381 - mse: 21934.8379 - val_loss: 1833563.2500 - val_mse: 1833563.2500\n",
      "Epoch 318/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 29017.7937 - mse: 29017.7949 - val_loss: 1830303.3750 - val_mse: 1830303.3750\n",
      "Epoch 319/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 19016.1442 - mse: 19016.1445 - val_loss: 1802045.3750 - val_mse: 1802045.3750\n",
      "Epoch 320/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 22261.4104 - mse: 22261.4102 - val_loss: 1772622.2500 - val_mse: 1772622.2500\n",
      "Epoch 321/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 28484.2462 - mse: 28484.2461 - val_loss: 1787296.1250 - val_mse: 1787296.1250\n",
      "Epoch 322/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 22582.3353 - mse: 22582.3359 - val_loss: 1807916.3750 - val_mse: 1807916.3750\n",
      "Epoch 323/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 20302.6789 - mse: 20302.6777 - val_loss: 1825435.2500 - val_mse: 1825435.2500\n",
      "Epoch 324/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 30930.7795 - mse: 30930.7812 - val_loss: 1815850.0000 - val_mse: 1815850.0000\n",
      "Epoch 325/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 18676.0392 - mse: 18676.0391 - val_loss: 1841757.5000 - val_mse: 1841757.5000\n",
      "Epoch 326/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 25091.5873 - mse: 25091.5879 - val_loss: 1880873.8750 - val_mse: 1880873.8750\n",
      "Epoch 327/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 25801.4614 - mse: 25801.4629 - val_loss: 1896926.1250 - val_mse: 1896926.1250\n",
      "Epoch 328/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 21372.1698 - mse: 21372.1680 - val_loss: 1895740.0000 - val_mse: 1895740.0000\n",
      "Epoch 329/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 23589.2554 - mse: 23589.2559 - val_loss: 1910873.3750 - val_mse: 1910873.3750\n",
      "Epoch 330/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 30550.3429 - mse: 30550.3438 - val_loss: 1954330.3750 - val_mse: 1954330.3750\n",
      "Epoch 331/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 27098.6976 - mse: 27098.6973 - val_loss: 1979509.0000 - val_mse: 1979509.0000\n",
      "Epoch 332/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 20710.3142 - mse: 20710.3145 - val_loss: 1992684.5000 - val_mse: 1992684.5000\n",
      "Epoch 333/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 25248.5341 - mse: 25248.5332 - val_loss: 1980311.6250 - val_mse: 1980311.6250\n",
      "Epoch 334/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 25645.8775 - mse: 25645.8770 - val_loss: 1978206.8750 - val_mse: 1978206.8750\n",
      "Epoch 335/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 22283.4342 - mse: 22283.4336 - val_loss: 1958031.3750 - val_mse: 1958031.3750\n",
      "Epoch 336/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 20727.7987 - mse: 20727.7988 - val_loss: 1945225.8750 - val_mse: 1945225.8750\n",
      "Epoch 337/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 22248.7948 - mse: 22248.7949 - val_loss: 1915818.3750 - val_mse: 1915818.3750\n",
      "Epoch 338/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 26412.7473 - mse: 26412.7480 - val_loss: 1870775.1250 - val_mse: 1870775.1250\n",
      "Epoch 339/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 20189.5566 - mse: 20189.5566 - val_loss: 1844255.5000 - val_mse: 1844255.5000\n",
      "Epoch 340/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 24680.3063 - mse: 24680.3066 - val_loss: 1836122.3750 - val_mse: 1836122.3750\n",
      "Epoch 341/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 26107.3477 - mse: 26107.3477 - val_loss: 1854806.3750 - val_mse: 1854806.3750\n",
      "Epoch 342/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 24507.4141 - mse: 24507.4121 - val_loss: 1859139.8750 - val_mse: 1859139.8750\n",
      "Epoch 343/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 25763.6436 - mse: 25763.6426 - val_loss: 1835398.2500 - val_mse: 1835398.2500\n",
      "Epoch 344/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 27386.3879 - mse: 27386.3887 - val_loss: 1793517.0000 - val_mse: 1793517.0000\n",
      "Epoch 345/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 24669.0695 - mse: 24669.0703 - val_loss: 1778427.7500 - val_mse: 1778427.7500\n",
      "Epoch 346/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 27417.6125 - mse: 27417.6133 - val_loss: 1765136.3750 - val_mse: 1765136.3750\n",
      "Epoch 347/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 27275.2953 - mse: 27275.2949 - val_loss: 1807159.0000 - val_mse: 1807159.0000\n",
      "Epoch 348/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 28863.1906 - mse: 28863.1914 - val_loss: 1850624.8750 - val_mse: 1850624.8750\n",
      "Epoch 349/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23711.8360 - mse: 23711.8359 - val_loss: 1861879.1250 - val_mse: 1861879.1250\n",
      "Epoch 350/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 22568.1829 - mse: 22568.1836 - val_loss: 1851976.1250 - val_mse: 1851976.1250\n",
      "Epoch 351/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 20827.5716 - mse: 20827.5723 - val_loss: 1825755.2500 - val_mse: 1825755.2500\n",
      "Epoch 352/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 24570.5286 - mse: 24570.5293 - val_loss: 1789231.6250 - val_mse: 1789231.6250\n",
      "Epoch 353/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 20972.8180 - mse: 20972.8164 - val_loss: 1793071.8750 - val_mse: 1793071.8750\n",
      "Epoch 354/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 22847.0348 - mse: 22847.0352 - val_loss: 1770243.5000 - val_mse: 1770243.5000\n",
      "Epoch 355/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 22638.5403 - mse: 22638.5410 - val_loss: 1735745.1250 - val_mse: 1735745.1250\n",
      "Epoch 356/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 20475.8051 - mse: 20475.8047 - val_loss: 1697632.1250 - val_mse: 1697632.1250\n",
      "Epoch 357/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 20145.8646 - mse: 20145.8652 - val_loss: 1711274.5000 - val_mse: 1711274.5000\n",
      "Epoch 358/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 26745.5288 - mse: 26745.5273 - val_loss: 1760652.2500 - val_mse: 1760652.2500\n",
      "Epoch 359/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 21893.4347 - mse: 21893.4336 - val_loss: 1829769.1250 - val_mse: 1829769.1250\n",
      "Epoch 360/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 21326.4316 - mse: 21326.4316 - val_loss: 1840906.5000 - val_mse: 1840906.5000\n",
      "Epoch 361/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 27197.8319 - mse: 27197.8320 - val_loss: 1835237.1250 - val_mse: 1835237.1250\n",
      "Epoch 362/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 25274.6374 - mse: 25274.6367 - val_loss: 1795801.1250 - val_mse: 1795801.1250\n",
      "Epoch 363/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 28166.4558 - mse: 28166.4551 - val_loss: 1837371.3750 - val_mse: 1837371.3750\n",
      "Epoch 364/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 26866.2846 - mse: 26866.2832 - val_loss: 1911393.8750 - val_mse: 1911393.8750\n",
      "Epoch 365/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 24812.8298 - mse: 24812.8301 - val_loss: 1946264.5000 - val_mse: 1946264.5000\n",
      "Epoch 366/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23882.7200 - mse: 23882.7188 - val_loss: 1896961.8750 - val_mse: 1896961.8750\n",
      "Epoch 367/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 20701.4055 - mse: 20701.4043 - val_loss: 1860480.5000 - val_mse: 1860480.5000\n",
      "Epoch 368/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 25349.5514 - mse: 25349.5527 - val_loss: 1865275.0000 - val_mse: 1865275.0000\n",
      "Epoch 369/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 28341.3290 - mse: 28341.3281 - val_loss: 1888925.0000 - val_mse: 1888925.0000\n",
      "Epoch 370/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 22875.9056 - mse: 22875.9062 - val_loss: 1939585.1250 - val_mse: 1939585.1250\n",
      "Epoch 371/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 25008.9383 - mse: 25008.9375 - val_loss: 1952517.8750 - val_mse: 1952517.8750\n",
      "Epoch 372/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 24322.3699 - mse: 24322.3691 - val_loss: 1911180.2500 - val_mse: 1911180.2500\n",
      "Epoch 373/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 21389.7850 - mse: 21389.7852 - val_loss: 1868139.6250 - val_mse: 1868139.6250\n",
      "Epoch 374/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 24110.4598 - mse: 24110.4590 - val_loss: 1814109.8750 - val_mse: 1814109.8750\n",
      "Epoch 375/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 23748.1181 - mse: 23748.1191 - val_loss: 1772659.7500 - val_mse: 1772659.7500\n",
      "Epoch 376/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 22124.5630 - mse: 22124.5625 - val_loss: 1786021.0000 - val_mse: 1786021.0000\n",
      "Epoch 377/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 22128.3880 - mse: 22128.3887 - val_loss: 1796914.6250 - val_mse: 1796914.6250\n",
      "Epoch 378/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 21248.6477 - mse: 21248.6484 - val_loss: 1785244.8750 - val_mse: 1785244.8750\n",
      "Epoch 379/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 23504.0322 - mse: 23504.0332 - val_loss: 1804139.0000 - val_mse: 1804139.0000\n",
      "Epoch 380/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 20477.0413 - mse: 20477.0410 - val_loss: 1823908.6250 - val_mse: 1823908.6250\n",
      "Epoch 381/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 26023.6861 - mse: 26023.6855 - val_loss: 1831526.7500 - val_mse: 1831526.7500\n",
      "Epoch 382/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 22849.9274 - mse: 22849.9258 - val_loss: 1840730.5000 - val_mse: 1840730.5000\n",
      "Epoch 383/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 23136.7491 - mse: 23136.7500 - val_loss: 1810873.1250 - val_mse: 1810873.1250\n",
      "Epoch 384/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 25496.4457 - mse: 25496.4453 - val_loss: 1762454.5000 - val_mse: 1762454.5000\n",
      "Epoch 385/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 20575.1180 - mse: 20575.1172 - val_loss: 1733025.8750 - val_mse: 1733025.8750\n",
      "Epoch 386/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 21837.0112 - mse: 21837.0098 - val_loss: 1702138.7500 - val_mse: 1702138.7500\n",
      "Epoch 387/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 19889.3596 - mse: 19889.3594 - val_loss: 1678331.7500 - val_mse: 1678331.7500\n",
      "Epoch 388/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23517.9738 - mse: 23517.9727 - val_loss: 1696200.5000 - val_mse: 1696200.5000\n",
      "Epoch 389/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 21999.0818 - mse: 21999.0820 - val_loss: 1740923.3750 - val_mse: 1740923.3750\n",
      "Epoch 390/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 24276.2871 - mse: 24276.2871 - val_loss: 1813863.5000 - val_mse: 1813863.5000\n",
      "Epoch 391/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 26610.3110 - mse: 26610.3125 - val_loss: 1811401.1250 - val_mse: 1811401.1250\n",
      "Epoch 392/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 25222.3808 - mse: 25222.3828 - val_loss: 1821944.6250 - val_mse: 1821944.6250\n",
      "Epoch 393/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 27996.2718 - mse: 27996.2715 - val_loss: 1854246.8750 - val_mse: 1854246.8750\n",
      "Epoch 394/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 26038.9550 - mse: 26038.9551 - val_loss: 1890215.2500 - val_mse: 1890215.2500\n",
      "Epoch 395/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 26818.9867 - mse: 26818.9863 - val_loss: 1922598.8750 - val_mse: 1922598.8750\n",
      "Epoch 396/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 21014.6426 - mse: 21014.6426 - val_loss: 1928522.3750 - val_mse: 1928522.3750\n",
      "Epoch 397/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 34310.6330 - mse: 34310.6328 - val_loss: 1934095.8750 - val_mse: 1934095.8750\n",
      "Epoch 398/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 20778.4637 - mse: 20778.4648 - val_loss: 1937444.6250 - val_mse: 1937444.6250\n",
      "Epoch 399/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 25273.1243 - mse: 25273.1250 - val_loss: 1905812.2500 - val_mse: 1905812.2500\n",
      "Epoch 400/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 21429.2518 - mse: 21429.2520 - val_loss: 1919121.3750 - val_mse: 1919121.3750\n",
      "Epoch 401/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 18052.0331 - mse: 18052.0332 - val_loss: 1920569.1250 - val_mse: 1920569.1250\n",
      "Epoch 402/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 25299.3815 - mse: 25299.3828 - val_loss: 1912260.7500 - val_mse: 1912260.7500\n",
      "Epoch 403/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 22568.0271 - mse: 22568.0273 - val_loss: 1884454.7500 - val_mse: 1884454.7500\n",
      "Epoch 404/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23441.7126 - mse: 23441.7129 - val_loss: 1850890.8750 - val_mse: 1850890.8750\n",
      "Epoch 405/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 33208.6310 - mse: 33208.6328 - val_loss: 1825337.3750 - val_mse: 1825337.3750\n",
      "Epoch 406/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 23738.7753 - mse: 23738.7754 - val_loss: 1837828.7500 - val_mse: 1837828.7500\n",
      "Epoch 407/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 28108.8314 - mse: 28108.8320 - val_loss: 1840294.6250 - val_mse: 1840294.6250\n",
      "Epoch 408/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 20368.0074 - mse: 20368.0078 - val_loss: 1827859.1250 - val_mse: 1827859.1250\n",
      "Epoch 409/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 25727.8185 - mse: 25727.8184 - val_loss: 1845085.6250 - val_mse: 1845085.6250\n",
      "Epoch 410/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 26968.3879 - mse: 26968.3887 - val_loss: 1808838.6250 - val_mse: 1808838.6250\n",
      "Epoch 411/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 27143.1916 - mse: 27143.1914 - val_loss: 1805792.0000 - val_mse: 1805792.0000\n",
      "Epoch 412/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 19644.3059 - mse: 19644.3066 - val_loss: 1814988.3750 - val_mse: 1814988.3750\n",
      "Epoch 413/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 23582.9130 - mse: 23582.9121 - val_loss: 1801319.3750 - val_mse: 1801319.3750\n",
      "Epoch 414/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 23833.5628 - mse: 23833.5625 - val_loss: 1807169.8750 - val_mse: 1807169.8750\n",
      "Epoch 415/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 21907.8073 - mse: 21907.8066 - val_loss: 1794717.8750 - val_mse: 1794717.8750\n",
      "Epoch 416/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 20191.5813 - mse: 20191.5820 - val_loss: 1804675.0000 - val_mse: 1804675.0000\n",
      "Epoch 417/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 25082.5285 - mse: 25082.5293 - val_loss: 1831091.6250 - val_mse: 1831091.6250\n",
      "Epoch 418/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 22833.0581 - mse: 22833.0586 - val_loss: 1845238.2500 - val_mse: 1845238.2500\n",
      "Epoch 419/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 30347.3117 - mse: 30347.3125 - val_loss: 1852016.2500 - val_mse: 1852016.2500\n",
      "Epoch 420/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 22864.2288 - mse: 22864.2305 - val_loss: 1839974.6250 - val_mse: 1839974.6250\n",
      "Epoch 421/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 24823.6342 - mse: 24823.6348 - val_loss: 1841522.1250 - val_mse: 1841522.1250\n",
      "Epoch 422/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 29394.3561 - mse: 29394.3574 - val_loss: 1860766.6250 - val_mse: 1860766.6250\n",
      "Epoch 423/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 28307.9849 - mse: 28307.9844 - val_loss: 1884108.2500 - val_mse: 1884108.2500\n",
      "Epoch 424/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 27318.3058 - mse: 27318.3066 - val_loss: 1875133.2500 - val_mse: 1875133.2500\n",
      "Epoch 425/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 23544.4426 - mse: 23544.4434 - val_loss: 1858534.7500 - val_mse: 1858534.7500\n",
      "Epoch 426/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 20345.1151 - mse: 20345.1152 - val_loss: 1817784.6250 - val_mse: 1817784.6250\n",
      "Epoch 427/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 24440.9855 - mse: 24440.9863 - val_loss: 1793921.1250 - val_mse: 1793921.1250\n",
      "Epoch 428/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 24902.4416 - mse: 24902.4414 - val_loss: 1822185.1250 - val_mse: 1822185.1250\n",
      "Epoch 429/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 26522.9270 - mse: 26522.9277 - val_loss: 1846817.5000 - val_mse: 1846817.5000\n",
      "Epoch 430/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 21290.0032 - mse: 21290.0039 - val_loss: 1838016.1250 - val_mse: 1838016.1250\n",
      "Epoch 431/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 23821.4632 - mse: 23821.4629 - val_loss: 1834363.0000 - val_mse: 1834363.0000\n",
      "Epoch 432/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 22562.3538 - mse: 22562.3535 - val_loss: 1843121.0000 - val_mse: 1843121.0000\n",
      "Epoch 433/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 24190.0747 - mse: 24190.0742 - val_loss: 1833571.8750 - val_mse: 1833571.8750\n",
      "Epoch 434/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 27483.2008 - mse: 27483.2012 - val_loss: 1855048.8750 - val_mse: 1855048.8750\n",
      "Epoch 435/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 24251.0547 - mse: 24251.0547 - val_loss: 1897341.7500 - val_mse: 1897341.7500\n",
      "Epoch 436/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 23115.2905 - mse: 23115.2891 - val_loss: 1890391.0000 - val_mse: 1890391.0000\n",
      "Epoch 437/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 20695.7276 - mse: 20695.7266 - val_loss: 1815323.2500 - val_mse: 1815323.2500\n",
      "Epoch 438/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23926.4375 - mse: 23926.4375 - val_loss: 1763423.3750 - val_mse: 1763423.3750\n",
      "Epoch 439/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 25241.1105 - mse: 25241.1113 - val_loss: 1757391.2500 - val_mse: 1757391.2500\n",
      "Epoch 440/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 25980.0958 - mse: 25980.0938 - val_loss: 1751763.0000 - val_mse: 1751763.0000\n",
      "Epoch 441/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 24394.9480 - mse: 24394.9492 - val_loss: 1729030.2500 - val_mse: 1729030.2500\n",
      "Epoch 442/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 29473.7339 - mse: 29473.7324 - val_loss: 1665786.8750 - val_mse: 1665786.8750\n",
      "Epoch 443/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 25184.3483 - mse: 25184.3496 - val_loss: 1606058.7500 - val_mse: 1606058.7500\n",
      "Epoch 444/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 24558.2830 - mse: 24558.2832 - val_loss: 1576245.5000 - val_mse: 1576245.5000\n",
      "Epoch 445/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 25308.8001 - mse: 25308.8008 - val_loss: 1577318.8750 - val_mse: 1577318.8750\n",
      "Epoch 446/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 21587.5592 - mse: 21587.5605 - val_loss: 1624968.2500 - val_mse: 1624968.2500\n",
      "Epoch 447/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 32946.2386 - mse: 32946.2383 - val_loss: 1645065.7500 - val_mse: 1645065.7500\n",
      "Epoch 448/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 27255.4146 - mse: 27255.4141 - val_loss: 1660219.8750 - val_mse: 1660219.8750\n",
      "Epoch 449/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 21818.2995 - mse: 21818.2988 - val_loss: 1671069.0000 - val_mse: 1671069.0000\n",
      "Epoch 450/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 22115.1435 - mse: 22115.1445 - val_loss: 1699576.3750 - val_mse: 1699576.3750\n",
      "Epoch 451/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 19079.7943 - mse: 19079.7949 - val_loss: 1700407.1250 - val_mse: 1700407.1250\n",
      "Epoch 452/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23653.8903 - mse: 23653.8887 - val_loss: 1657636.7500 - val_mse: 1657636.7500\n",
      "Epoch 453/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 21612.8757 - mse: 21612.8750 - val_loss: 1626301.2500 - val_mse: 1626301.2500\n",
      "Epoch 454/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 21383.8634 - mse: 21383.8652 - val_loss: 1613591.7500 - val_mse: 1613591.7500\n",
      "Epoch 455/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 24857.0183 - mse: 24857.0176 - val_loss: 1635834.5000 - val_mse: 1635834.5000\n",
      "Epoch 456/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 17124.2402 - mse: 17124.2402 - val_loss: 1642012.7500 - val_mse: 1642012.7500\n",
      "Epoch 457/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 23828.2483 - mse: 23828.2480 - val_loss: 1699146.0000 - val_mse: 1699146.0000\n",
      "Epoch 458/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 18951.8635 - mse: 18951.8652 - val_loss: 1744241.7500 - val_mse: 1744241.7500\n",
      "Epoch 459/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 19975.1616 - mse: 19975.1621 - val_loss: 1765563.2500 - val_mse: 1765563.2500\n",
      "Epoch 460/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 25267.0589 - mse: 25267.0586 - val_loss: 1751438.2500 - val_mse: 1751438.2500\n",
      "Epoch 461/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 25341.6748 - mse: 25341.6758 - val_loss: 1706308.2500 - val_mse: 1706308.2500\n",
      "Epoch 462/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 21631.5116 - mse: 21631.5117 - val_loss: 1685043.0000 - val_mse: 1685043.0000\n",
      "Epoch 463/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 20498.7807 - mse: 20498.7812 - val_loss: 1674323.1250 - val_mse: 1674323.1250\n",
      "Epoch 464/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 22877.6129 - mse: 22877.6133 - val_loss: 1679271.2500 - val_mse: 1679271.2500\n",
      "Epoch 465/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 25337.1232 - mse: 25337.1230 - val_loss: 1656475.2500 - val_mse: 1656475.2500\n",
      "Epoch 466/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 19791.4663 - mse: 19791.4668 - val_loss: 1635710.1250 - val_mse: 1635710.1250\n",
      "Epoch 467/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 19150.5879 - mse: 19150.5879 - val_loss: 1642819.6250 - val_mse: 1642819.6250\n",
      "Epoch 468/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23344.7064 - mse: 23344.7070 - val_loss: 1656175.3750 - val_mse: 1656175.3750\n",
      "Epoch 469/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23812.4315 - mse: 23812.4316 - val_loss: 1644315.8750 - val_mse: 1644315.8750\n",
      "Epoch 470/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 23022.3467 - mse: 23022.3477 - val_loss: 1639831.8750 - val_mse: 1639831.8750\n",
      "Epoch 471/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 24314.1172 - mse: 24314.1172 - val_loss: 1654351.7500 - val_mse: 1654351.7500\n",
      "Epoch 472/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 23251.1696 - mse: 23251.1699 - val_loss: 1656009.2500 - val_mse: 1656009.2500\n",
      "Epoch 473/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 20262.7226 - mse: 20262.7227 - val_loss: 1647144.8750 - val_mse: 1647144.8750\n",
      "Epoch 474/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 20595.7293 - mse: 20595.7285 - val_loss: 1662030.8750 - val_mse: 1662030.8750\n",
      "Epoch 475/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 19688.9335 - mse: 19688.9316 - val_loss: 1679957.2500 - val_mse: 1679957.2500\n",
      "Epoch 476/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 25138.9497 - mse: 25138.9492 - val_loss: 1674485.8750 - val_mse: 1674485.8750\n",
      "Epoch 477/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 20430.5530 - mse: 20430.5547 - val_loss: 1669060.8750 - val_mse: 1669060.8750\n",
      "Epoch 478/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 19248.7207 - mse: 19248.7207 - val_loss: 1665409.6250 - val_mse: 1665409.6250\n",
      "Epoch 479/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 22189.2170 - mse: 22189.2148 - val_loss: 1675658.0000 - val_mse: 1675658.0000\n",
      "Epoch 480/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 19774.6724 - mse: 19774.6738 - val_loss: 1693599.8750 - val_mse: 1693599.8750\n",
      "Epoch 481/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 16258.7253 - mse: 16258.7246 - val_loss: 1715261.7500 - val_mse: 1715261.7500\n",
      "Epoch 482/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 20513.0324 - mse: 20513.0332 - val_loss: 1716159.5000 - val_mse: 1716159.5000\n",
      "Epoch 483/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 21923.2481 - mse: 21923.2480 - val_loss: 1693084.5000 - val_mse: 1693084.5000\n",
      "Epoch 484/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 22920.3947 - mse: 22920.3945 - val_loss: 1725877.3750 - val_mse: 1725877.3750\n",
      "Epoch 485/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23898.1110 - mse: 23898.1113 - val_loss: 1651265.7500 - val_mse: 1651265.7500\n",
      "Epoch 486/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 24416.9340 - mse: 24416.9336 - val_loss: 1627787.5000 - val_mse: 1627787.5000\n",
      "Epoch 487/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 26484.2824 - mse: 26484.2832 - val_loss: 1599286.7500 - val_mse: 1599286.7500\n",
      "Epoch 488/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 22466.3957 - mse: 22466.3945 - val_loss: 1590405.7500 - val_mse: 1590405.7500\n",
      "Epoch 489/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 28504.6501 - mse: 28504.6484 - val_loss: 1618099.1250 - val_mse: 1618099.1250\n",
      "Epoch 490/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 25284.6834 - mse: 25284.6836 - val_loss: 1652533.8750 - val_mse: 1652533.8750\n",
      "Epoch 491/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 21177.9795 - mse: 21177.9785 - val_loss: 1678947.0000 - val_mse: 1678947.0000\n",
      "Epoch 492/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 27072.8724 - mse: 27072.8730 - val_loss: 1668832.5000 - val_mse: 1668832.5000\n",
      "Epoch 493/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 29693.4958 - mse: 29693.4941 - val_loss: 1664403.0000 - val_mse: 1664403.0000\n",
      "Epoch 494/1000\n",
      "1948/1948 [==============================] - 0s 16us/sample - loss: 21629.5417 - mse: 21629.5430 - val_loss: 1659899.8750 - val_mse: 1659899.8750\n",
      "Epoch 495/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23682.0211 - mse: 23682.0215 - val_loss: 1653495.2500 - val_mse: 1653495.2500\n",
      "Epoch 496/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23162.8990 - mse: 23162.8984 - val_loss: 1626527.2500 - val_mse: 1626527.2500\n",
      "Epoch 497/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 27037.1271 - mse: 27037.1270 - val_loss: 1574360.7500 - val_mse: 1574360.7500\n",
      "Epoch 498/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 21863.7167 - mse: 21863.7168 - val_loss: 1571092.6250 - val_mse: 1571092.6250\n",
      "Epoch 499/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 32778.5025 - mse: 32778.5000 - val_loss: 1663641.5000 - val_mse: 1663641.5000\n",
      "Epoch 500/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 28135.5702 - mse: 28135.5684 - val_loss: 1748871.5000 - val_mse: 1748871.5000\n",
      "Epoch 501/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 24223.0373 - mse: 24223.0371 - val_loss: 1822379.3750 - val_mse: 1822379.3750\n",
      "Epoch 502/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 27669.8064 - mse: 27669.8066 - val_loss: 1820535.2500 - val_mse: 1820535.2500\n",
      "Epoch 503/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 21307.5288 - mse: 21307.5273 - val_loss: 1840782.8750 - val_mse: 1840782.8750\n",
      "Epoch 504/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 25647.4463 - mse: 25647.4453 - val_loss: 1831973.0000 - val_mse: 1831973.0000\n",
      "Epoch 505/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23052.6649 - mse: 23052.6660 - val_loss: 1806791.2500 - val_mse: 1806791.2500\n",
      "Epoch 506/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 21215.3614 - mse: 21215.3613 - val_loss: 1769577.1250 - val_mse: 1769577.1250\n",
      "Epoch 507/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 22286.0053 - mse: 22286.0039 - val_loss: 1725116.6250 - val_mse: 1725116.6250\n",
      "Epoch 508/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 19650.2786 - mse: 19650.2793 - val_loss: 1679718.5000 - val_mse: 1679718.5000\n",
      "Epoch 509/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 21757.3434 - mse: 21757.3438 - val_loss: 1658460.6250 - val_mse: 1658460.6250\n",
      "Epoch 510/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 20205.0217 - mse: 20205.0234 - val_loss: 1670090.6250 - val_mse: 1670090.6250\n",
      "Epoch 511/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 20380.4372 - mse: 20380.4375 - val_loss: 1690907.2500 - val_mse: 1690907.2500\n",
      "Epoch 512/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 17150.9291 - mse: 17150.9297 - val_loss: 1709475.5000 - val_mse: 1709475.5000\n",
      "Epoch 513/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 21350.4055 - mse: 21350.4062 - val_loss: 1714752.7500 - val_mse: 1714752.7500\n",
      "Epoch 514/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 22019.8862 - mse: 22019.8867 - val_loss: 1693145.1250 - val_mse: 1693145.1250\n",
      "Epoch 515/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 24628.0985 - mse: 24628.0977 - val_loss: 1680051.6250 - val_mse: 1680051.6250\n",
      "Epoch 516/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 19027.1024 - mse: 19027.1035 - val_loss: 1647307.2500 - val_mse: 1647307.2500\n",
      "Epoch 517/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 22629.3413 - mse: 22629.3418 - val_loss: 1630341.1250 - val_mse: 1630341.1250\n",
      "Epoch 518/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 22718.2366 - mse: 22718.2363 - val_loss: 1626481.1250 - val_mse: 1626481.1250\n",
      "Epoch 519/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 21853.9729 - mse: 21853.9727 - val_loss: 1614175.3750 - val_mse: 1614175.3750\n",
      "Epoch 520/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23268.9046 - mse: 23268.9043 - val_loss: 1686120.1250 - val_mse: 1686120.1250\n",
      "Epoch 521/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 23516.4607 - mse: 23516.4590 - val_loss: 1735409.2500 - val_mse: 1735409.2500\n",
      "Epoch 522/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 22725.7960 - mse: 22725.7949 - val_loss: 1757009.2500 - val_mse: 1757009.2500\n",
      "Epoch 523/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 25537.3749 - mse: 25537.3750 - val_loss: 1754729.0000 - val_mse: 1754729.0000\n",
      "Epoch 524/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 24028.7769 - mse: 24028.7754 - val_loss: 1728639.1250 - val_mse: 1728639.1250\n",
      "Epoch 525/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 20541.9557 - mse: 20541.9551 - val_loss: 1749614.6250 - val_mse: 1749614.6250\n",
      "Epoch 526/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 32308.9927 - mse: 32308.9941 - val_loss: 1799965.8750 - val_mse: 1799965.8750\n",
      "Epoch 527/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 27284.7370 - mse: 27284.7344 - val_loss: 1868164.8750 - val_mse: 1868164.8750\n",
      "Epoch 528/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 20935.2505 - mse: 20935.2500 - val_loss: 1916450.5000 - val_mse: 1916450.5000\n",
      "Epoch 529/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 27452.2443 - mse: 27452.2441 - val_loss: 1940708.3750 - val_mse: 1940708.3750\n",
      "Epoch 530/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 22472.1286 - mse: 22472.1270 - val_loss: 1963233.0000 - val_mse: 1963233.0000\n",
      "Epoch 531/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 20109.1277 - mse: 20109.1270 - val_loss: 1970334.5000 - val_mse: 1970334.5000\n",
      "Epoch 532/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 33358.9833 - mse: 33358.9805 - val_loss: 1967719.8750 - val_mse: 1967719.8750\n",
      "Epoch 533/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 22030.6132 - mse: 22030.6113 - val_loss: 1960770.1250 - val_mse: 1960770.1250\n",
      "Epoch 534/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 24712.7528 - mse: 24712.7539 - val_loss: 1948316.1250 - val_mse: 1948316.1250\n",
      "Epoch 535/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 28160.9030 - mse: 28160.9043 - val_loss: 1938877.7500 - val_mse: 1938877.7500\n",
      "Epoch 536/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 24329.0748 - mse: 24329.0762 - val_loss: 1960000.1250 - val_mse: 1960000.1250\n",
      "Epoch 537/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23980.2772 - mse: 23980.2773 - val_loss: 2007586.2500 - val_mse: 2007586.2500\n",
      "Epoch 538/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 24424.0783 - mse: 24424.0781 - val_loss: 2057622.1250 - val_mse: 2057622.1250\n",
      "Epoch 539/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23018.2723 - mse: 23018.2715 - val_loss: 2095175.5000 - val_mse: 2095175.5000\n",
      "Epoch 540/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 25516.7210 - mse: 25516.7207 - val_loss: 2070785.0000 - val_mse: 2070785.0000\n",
      "Epoch 541/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 25123.7125 - mse: 25123.7129 - val_loss: 2045627.6250 - val_mse: 2045627.6250\n",
      "Epoch 542/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23169.2524 - mse: 23169.2520 - val_loss: 1998429.6250 - val_mse: 1998429.6250\n",
      "Epoch 543/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23408.4178 - mse: 23408.4180 - val_loss: 1962352.6250 - val_mse: 1962352.6250\n",
      "Epoch 544/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 19507.0578 - mse: 19507.0566 - val_loss: 1924987.7500 - val_mse: 1924987.7500\n",
      "Epoch 545/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 26192.4997 - mse: 26192.5020 - val_loss: 1897116.7500 - val_mse: 1897116.7500\n",
      "Epoch 546/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 20906.8929 - mse: 20906.8926 - val_loss: 1862374.7500 - val_mse: 1862374.7500\n",
      "Epoch 547/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 21553.3569 - mse: 21553.3574 - val_loss: 1853329.2500 - val_mse: 1853329.2500\n",
      "Epoch 548/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 25010.7152 - mse: 25010.7148 - val_loss: 1937363.1250 - val_mse: 1937363.1250\n",
      "Epoch 549/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 28428.1983 - mse: 28428.1973 - val_loss: 2009325.2500 - val_mse: 2009325.2500\n",
      "Epoch 550/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 24006.2226 - mse: 24006.2227 - val_loss: 1989507.5000 - val_mse: 1989507.5000\n",
      "Epoch 551/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 21304.6458 - mse: 21304.6445 - val_loss: 1998979.6250 - val_mse: 1998979.6250\n",
      "Epoch 552/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 26044.4811 - mse: 26044.4805 - val_loss: 1970626.1250 - val_mse: 1970626.1250\n",
      "Epoch 553/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 21021.5388 - mse: 21021.5410 - val_loss: 1979550.1250 - val_mse: 1979550.1250\n",
      "Epoch 554/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 22298.8326 - mse: 22298.8340 - val_loss: 1991275.6250 - val_mse: 1991275.6250\n",
      "Epoch 555/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 29460.0114 - mse: 29460.0117 - val_loss: 1997032.0000 - val_mse: 1997032.0000\n",
      "Epoch 556/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 27770.0326 - mse: 27770.0332 - val_loss: 2032099.8750 - val_mse: 2032099.8750\n",
      "Epoch 557/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 24114.6732 - mse: 24114.6738 - val_loss: 2031473.7500 - val_mse: 2031473.7500\n",
      "Epoch 558/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 30038.1578 - mse: 30038.1582 - val_loss: 2035927.6250 - val_mse: 2035927.6250\n",
      "Epoch 559/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 20064.7559 - mse: 20064.7559 - val_loss: 1990838.8750 - val_mse: 1990838.8750\n",
      "Epoch 560/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 26081.3187 - mse: 26081.3184 - val_loss: 1971791.5000 - val_mse: 1971791.5000\n",
      "Epoch 561/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 27523.1554 - mse: 27523.1543 - val_loss: 1981612.6250 - val_mse: 1981612.6250\n",
      "Epoch 562/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 36193.2921 - mse: 36193.2891 - val_loss: 1975104.8750 - val_mse: 1975104.8750\n",
      "Epoch 563/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 29710.9938 - mse: 29710.9941 - val_loss: 1944426.3750 - val_mse: 1944426.3750\n",
      "Epoch 564/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 24165.5425 - mse: 24165.5430 - val_loss: 2016073.8750 - val_mse: 2016073.8750\n",
      "Epoch 565/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23820.1749 - mse: 23820.1738 - val_loss: 2012486.5000 - val_mse: 2012486.5000\n",
      "Epoch 566/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 20378.5013 - mse: 20378.5020 - val_loss: 1931155.7500 - val_mse: 1931155.7500\n",
      "Epoch 567/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 24548.6938 - mse: 24548.6934 - val_loss: 1912791.6250 - val_mse: 1912791.6250\n",
      "Epoch 568/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 25596.5063 - mse: 25596.5059 - val_loss: 1896268.3750 - val_mse: 1896268.3750\n",
      "Epoch 569/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 22257.8703 - mse: 22257.8711 - val_loss: 1888207.5000 - val_mse: 1888207.5000\n",
      "Epoch 570/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 24996.6300 - mse: 24996.6309 - val_loss: 1888916.5000 - val_mse: 1888916.5000\n",
      "Epoch 571/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 20851.5858 - mse: 20851.5859 - val_loss: 1909640.0000 - val_mse: 1909640.0000\n",
      "Epoch 572/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 22355.3265 - mse: 22355.3262 - val_loss: 1956473.6250 - val_mse: 1956473.6250\n",
      "Epoch 573/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23925.7861 - mse: 23925.7871 - val_loss: 1986510.5000 - val_mse: 1986510.5000\n",
      "Epoch 574/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 26559.3680 - mse: 26559.3672 - val_loss: 1979337.8750 - val_mse: 1979337.8750\n",
      "Epoch 575/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 32811.7950 - mse: 32811.7930 - val_loss: 1906831.7500 - val_mse: 1906831.7500\n",
      "Epoch 576/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23121.0561 - mse: 23121.0547 - val_loss: 1849736.7500 - val_mse: 1849736.7500\n",
      "Epoch 577/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 21172.2213 - mse: 21172.2227 - val_loss: 1828245.0000 - val_mse: 1828245.0000\n",
      "Epoch 578/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 21833.2663 - mse: 21833.2656 - val_loss: 1815266.5000 - val_mse: 1815266.5000\n",
      "Epoch 579/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 24143.9997 - mse: 24144.0000 - val_loss: 1805793.5000 - val_mse: 1805793.5000\n",
      "Epoch 580/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 19218.5427 - mse: 19218.5430 - val_loss: 1799789.0000 - val_mse: 1799789.0000\n",
      "Epoch 581/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 23738.8132 - mse: 23738.8125 - val_loss: 1799880.8750 - val_mse: 1799880.8750\n",
      "Epoch 582/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 20074.5698 - mse: 20074.5703 - val_loss: 1807882.8750 - val_mse: 1807882.8750\n",
      "Epoch 583/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 26143.8991 - mse: 26143.8984 - val_loss: 1794505.0000 - val_mse: 1794505.0000\n",
      "Epoch 584/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 21438.6130 - mse: 21438.6113 - val_loss: 1798276.7500 - val_mse: 1798276.7500\n",
      "Epoch 585/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23612.1410 - mse: 23612.1387 - val_loss: 1804432.2500 - val_mse: 1804432.2500\n",
      "Epoch 586/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 21906.3713 - mse: 21906.3711 - val_loss: 1751080.5000 - val_mse: 1751080.5000\n",
      "Epoch 587/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 31151.3851 - mse: 31151.3848 - val_loss: 1698574.8750 - val_mse: 1698574.8750\n",
      "Epoch 588/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 22385.7122 - mse: 22385.7129 - val_loss: 1731818.1250 - val_mse: 1731818.1250\n",
      "Epoch 589/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 29002.4762 - mse: 29002.4766 - val_loss: 1744229.2500 - val_mse: 1744229.2500\n",
      "Epoch 590/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 26009.1781 - mse: 26009.1777 - val_loss: 1800789.8750 - val_mse: 1800789.8750\n",
      "Epoch 591/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23337.7330 - mse: 23337.7324 - val_loss: 1840877.0000 - val_mse: 1840877.0000\n",
      "Epoch 592/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 26429.8912 - mse: 26429.8906 - val_loss: 1882821.1250 - val_mse: 1882821.1250\n",
      "Epoch 593/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 26183.8151 - mse: 26183.8145 - val_loss: 1902479.0000 - val_mse: 1902479.0000\n",
      "Epoch 594/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 26611.5694 - mse: 26611.5684 - val_loss: 1890533.1250 - val_mse: 1890533.1250\n",
      "Epoch 595/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23063.7087 - mse: 23063.7090 - val_loss: 1850318.0000 - val_mse: 1850318.0000\n",
      "Epoch 596/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 22297.5788 - mse: 22297.5781 - val_loss: 1828095.7500 - val_mse: 1828095.7500\n",
      "Epoch 597/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 20951.2852 - mse: 20951.2852 - val_loss: 1782984.8750 - val_mse: 1782984.8750\n",
      "Epoch 598/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 33264.7974 - mse: 33264.7969 - val_loss: 1773224.0000 - val_mse: 1773224.0000\n",
      "Epoch 599/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 22521.9383 - mse: 22521.9375 - val_loss: 1764602.5000 - val_mse: 1764602.5000\n",
      "Epoch 600/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 19121.8986 - mse: 19121.8984 - val_loss: 1790224.5000 - val_mse: 1790224.5000\n",
      "Epoch 601/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 27300.3816 - mse: 27300.3828 - val_loss: 1864945.2500 - val_mse: 1864945.2500\n",
      "Epoch 602/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23992.3669 - mse: 23992.3672 - val_loss: 1958673.1250 - val_mse: 1958673.1250\n",
      "Epoch 603/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 22082.7985 - mse: 22082.7988 - val_loss: 2021277.0000 - val_mse: 2021277.0000\n",
      "Epoch 604/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 18688.2896 - mse: 18688.2891 - val_loss: 2048531.0000 - val_mse: 2048531.0000\n",
      "Epoch 605/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 26622.3004 - mse: 26622.2988 - val_loss: 2061947.7500 - val_mse: 2061947.7500\n",
      "Epoch 606/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 29610.8055 - mse: 29610.8047 - val_loss: 2046690.5000 - val_mse: 2046690.5000\n",
      "Epoch 607/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 29273.7125 - mse: 29273.7129 - val_loss: 1987198.3750 - val_mse: 1987198.3750\n",
      "Epoch 608/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 20770.6573 - mse: 20770.6562 - val_loss: 1926719.3750 - val_mse: 1926719.3750\n",
      "Epoch 609/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 28881.3886 - mse: 28881.3887 - val_loss: 1900323.5000 - val_mse: 1900323.5000\n",
      "Epoch 610/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 32138.0859 - mse: 32138.0859 - val_loss: 1884698.7500 - val_mse: 1884698.7500\n",
      "Epoch 611/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 22934.7792 - mse: 22934.7812 - val_loss: 1899823.7500 - val_mse: 1899823.7500\n",
      "Epoch 612/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 27142.9341 - mse: 27142.9336 - val_loss: 1947720.0000 - val_mse: 1947720.0000\n",
      "Epoch 613/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 19544.8092 - mse: 19544.8086 - val_loss: 1981581.2500 - val_mse: 1981581.2500\n",
      "Epoch 614/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 22598.7015 - mse: 22598.7031 - val_loss: 1982939.2500 - val_mse: 1982939.2500\n",
      "Epoch 615/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23993.7186 - mse: 23993.7188 - val_loss: 1972595.5000 - val_mse: 1972595.5000\n",
      "Epoch 616/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23134.7811 - mse: 23134.7812 - val_loss: 1958005.2500 - val_mse: 1958005.2500\n",
      "Epoch 617/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 27665.7013 - mse: 27665.7012 - val_loss: 1953998.1250 - val_mse: 1953998.1250\n",
      "Epoch 618/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 26459.3915 - mse: 26459.3926 - val_loss: 1941354.5000 - val_mse: 1941354.5000\n",
      "Epoch 619/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 23880.9756 - mse: 23880.9746 - val_loss: 1931502.5000 - val_mse: 1931502.5000\n",
      "Epoch 620/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 20912.5880 - mse: 20912.5879 - val_loss: 1916052.2500 - val_mse: 1916052.2500\n",
      "Epoch 621/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 20782.6949 - mse: 20782.6934 - val_loss: 1936537.6250 - val_mse: 1936537.6250\n",
      "Epoch 622/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 24550.3599 - mse: 24550.3613 - val_loss: 1983698.0000 - val_mse: 1983698.0000\n",
      "Epoch 623/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 24657.9018 - mse: 24657.9023 - val_loss: 2009930.3750 - val_mse: 2009930.3750\n",
      "Epoch 624/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 22170.6133 - mse: 22170.6113 - val_loss: 1990350.2500 - val_mse: 1990350.2500\n",
      "Epoch 625/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 22412.5172 - mse: 22412.5176 - val_loss: 1945645.1250 - val_mse: 1945645.1250\n",
      "Epoch 626/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 24757.0426 - mse: 24757.0430 - val_loss: 1865830.7500 - val_mse: 1865830.7500\n",
      "Epoch 627/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 22555.0131 - mse: 22555.0137 - val_loss: 1834790.2500 - val_mse: 1834790.2500\n",
      "Epoch 628/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 27062.1831 - mse: 27062.1836 - val_loss: 1832280.0000 - val_mse: 1832280.0000\n",
      "Epoch 629/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 24643.5860 - mse: 24643.5859 - val_loss: 1831679.2500 - val_mse: 1831679.2500\n",
      "Epoch 630/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23818.4408 - mse: 23818.4395 - val_loss: 1826820.8750 - val_mse: 1826820.8750\n",
      "Epoch 631/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 19135.9283 - mse: 19135.9277 - val_loss: 1816917.3750 - val_mse: 1816917.3750\n",
      "Epoch 632/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 22316.8415 - mse: 22316.8418 - val_loss: 1829102.0000 - val_mse: 1829102.0000\n",
      "Epoch 633/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 28057.4324 - mse: 28057.4336 - val_loss: 1843211.6250 - val_mse: 1843211.6250\n",
      "Epoch 634/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 21155.9513 - mse: 21155.9512 - val_loss: 1854340.2500 - val_mse: 1854340.2500\n",
      "Epoch 635/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 25810.1628 - mse: 25810.1641 - val_loss: 1816161.8750 - val_mse: 1816161.8750\n",
      "Epoch 636/1000\n",
      "1948/1948 [==============================] - 0s 10us/sample - loss: 24100.0037 - mse: 24100.0039 - val_loss: 1780490.6250 - val_mse: 1780490.6250\n",
      "Epoch 637/1000\n",
      "1948/1948 [==============================] - 0s 7us/sample - loss: 19938.1322 - mse: 19938.1309 - val_loss: 1802173.8750 - val_mse: 1802173.8750\n",
      "Epoch 638/1000\n",
      "1948/1948 [==============================] - 0s 7us/sample - loss: 24831.2909 - mse: 24831.2891 - val_loss: 1802138.1250 - val_mse: 1802138.1250\n",
      "Epoch 639/1000\n",
      "1948/1948 [==============================] - 0s 7us/sample - loss: 20933.5999 - mse: 20933.5996 - val_loss: 1809615.6250 - val_mse: 1809615.6250\n",
      "Epoch 640/1000\n",
      "1948/1948 [==============================] - 0s 7us/sample - loss: 21216.1162 - mse: 21216.1152 - val_loss: 1797196.5000 - val_mse: 1797196.5000\n",
      "Epoch 641/1000\n",
      "1948/1948 [==============================] - ETA: 0s - loss: 25407.1777 - mse: 25407.177 - 0s 7us/sample - loss: 30872.4469 - mse: 30872.4473 - val_loss: 1796596.0000 - val_mse: 1796596.0000\n",
      "Epoch 642/1000\n",
      "1948/1948 [==============================] - 0s 7us/sample - loss: 22006.6867 - mse: 22006.6855 - val_loss: 1767153.3750 - val_mse: 1767153.3750\n",
      "Epoch 643/1000\n",
      "1948/1948 [==============================] - 0s 7us/sample - loss: 20632.3211 - mse: 20632.3203 - val_loss: 1741602.2500 - val_mse: 1741602.2500\n",
      "Epoch 644/1000\n",
      "1948/1948 [==============================] - 0s 7us/sample - loss: 24541.9823 - mse: 24541.9844 - val_loss: 1718818.1250 - val_mse: 1718818.1250\n",
      "Epoch 645/1000\n",
      "1948/1948 [==============================] - 0s 7us/sample - loss: 18267.7810 - mse: 18267.7832 - val_loss: 1700403.6250 - val_mse: 1700403.6250\n",
      "Epoch 646/1000\n",
      "1948/1948 [==============================] - 0s 8us/sample - loss: 27668.3569 - mse: 27668.3574 - val_loss: 1752027.7500 - val_mse: 1752027.7500\n",
      "Epoch 647/1000\n",
      "1948/1948 [==============================] - 0s 10us/sample - loss: 26527.6906 - mse: 26527.6914 - val_loss: 1784427.0000 - val_mse: 1784427.0000\n",
      "Epoch 648/1000\n",
      "1948/1948 [==============================] - 0s 10us/sample - loss: 23525.2958 - mse: 23525.2949 - val_loss: 1825079.0000 - val_mse: 1825079.0000\n",
      "Epoch 649/1000\n",
      "1948/1948 [==============================] - 0s 10us/sample - loss: 22129.5610 - mse: 22129.5605 - val_loss: 1837050.7500 - val_mse: 1837050.7500\n",
      "Epoch 650/1000\n",
      "1948/1948 [==============================] - 0s 11us/sample - loss: 23109.2797 - mse: 23109.2793 - val_loss: 1856073.1250 - val_mse: 1856073.1250\n",
      "Epoch 651/1000\n",
      "1948/1948 [==============================] - 0s 10us/sample - loss: 24317.0883 - mse: 24317.0879 - val_loss: 1874775.6250 - val_mse: 1874775.6250\n",
      "Epoch 652/1000\n",
      "1948/1948 [==============================] - 0s 10us/sample - loss: 24814.7925 - mse: 24814.7930 - val_loss: 1910963.8750 - val_mse: 1910963.8750\n",
      "Epoch 653/1000\n",
      "1948/1948 [==============================] - 0s 11us/sample - loss: 19120.8835 - mse: 19120.8828 - val_loss: 1924958.5000 - val_mse: 1924958.5000\n",
      "Epoch 654/1000\n",
      "1948/1948 [==============================] - 0s 10us/sample - loss: 18787.6689 - mse: 18787.6699 - val_loss: 1936543.8750 - val_mse: 1936543.8750\n",
      "Epoch 655/1000\n",
      "1948/1948 [==============================] - 0s 10us/sample - loss: 24242.5527 - mse: 24242.5527 - val_loss: 1961026.1250 - val_mse: 1961026.1250\n",
      "Epoch 656/1000\n",
      "1948/1948 [==============================] - 0s 10us/sample - loss: 17966.4301 - mse: 17966.4316 - val_loss: 1999720.7500 - val_mse: 1999720.7500\n",
      "Epoch 657/1000\n",
      "1948/1948 [==============================] - 0s 10us/sample - loss: 22626.7774 - mse: 22626.7754 - val_loss: 2030201.2500 - val_mse: 2030201.2500\n",
      "Epoch 658/1000\n",
      "1948/1948 [==============================] - 0s 10us/sample - loss: 26501.2061 - mse: 26501.2070 - val_loss: 2040913.0000 - val_mse: 2040913.0000\n",
      "Epoch 659/1000\n",
      "1948/1948 [==============================] - 0s 10us/sample - loss: 26573.1144 - mse: 26573.1133 - val_loss: 2048689.1250 - val_mse: 2048689.1250\n",
      "Epoch 660/1000\n",
      "1948/1948 [==============================] - 0s 11us/sample - loss: 21840.9196 - mse: 21840.9199 - val_loss: 2057733.2500 - val_mse: 2057733.2500\n",
      "Epoch 661/1000\n",
      "1948/1948 [==============================] - 0s 11us/sample - loss: 26000.8972 - mse: 26000.8965 - val_loss: 2028575.8750 - val_mse: 2028575.8750\n",
      "Epoch 662/1000\n",
      "1948/1948 [==============================] - 0s 11us/sample - loss: 24611.5376 - mse: 24611.5371 - val_loss: 1993587.3750 - val_mse: 1993587.3750\n",
      "Epoch 663/1000\n",
      "1948/1948 [==============================] - 0s 11us/sample - loss: 25945.5279 - mse: 25945.5273 - val_loss: 1965391.3750 - val_mse: 1965391.3750\n",
      "Epoch 664/1000\n",
      "1948/1948 [==============================] - 0s 12us/sample - loss: 20381.1678 - mse: 20381.1660 - val_loss: 1949879.2500 - val_mse: 1949879.2500\n",
      "Epoch 665/1000\n",
      "1948/1948 [==============================] - 0s 12us/sample - loss: 28919.1098 - mse: 28919.1094 - val_loss: 1953882.1250 - val_mse: 1953882.1250\n",
      "Epoch 666/1000\n",
      "1948/1948 [==============================] - 0s 11us/sample - loss: 23415.3503 - mse: 23415.3516 - val_loss: 1942932.3750 - val_mse: 1942932.3750\n",
      "Epoch 667/1000\n",
      "1948/1948 [==============================] - 0s 12us/sample - loss: 28605.2390 - mse: 28605.2402 - val_loss: 1935053.0000 - val_mse: 1935053.0000\n",
      "Epoch 668/1000\n",
      "1948/1948 [==============================] - 0s 12us/sample - loss: 21703.0527 - mse: 21703.0508 - val_loss: 1907286.1250 - val_mse: 1907286.1250\n",
      "Epoch 669/1000\n",
      "1948/1948 [==============================] - 0s 12us/sample - loss: 20733.3984 - mse: 20733.3984 - val_loss: 1877527.0000 - val_mse: 1877527.0000\n",
      "Epoch 670/1000\n",
      "1948/1948 [==============================] - 0s 11us/sample - loss: 23076.3490 - mse: 23076.3496 - val_loss: 1859555.0000 - val_mse: 1859555.0000\n",
      "Epoch 671/1000\n",
      "1948/1948 [==============================] - 0s 12us/sample - loss: 24093.8266 - mse: 24093.8281 - val_loss: 1827739.6250 - val_mse: 1827739.6250\n",
      "Epoch 672/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 22576.7467 - mse: 22576.7480 - val_loss: 1803044.6250 - val_mse: 1803044.6250\n",
      "Epoch 673/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23979.0489 - mse: 23979.0488 - val_loss: 1794038.7500 - val_mse: 1794038.7500\n",
      "Epoch 674/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 22728.3598 - mse: 22728.3594 - val_loss: 1786594.3750 - val_mse: 1786594.3750\n",
      "Epoch 675/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 18901.5941 - mse: 18901.5938 - val_loss: 1779871.2500 - val_mse: 1779871.2500\n",
      "Epoch 676/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 20561.0854 - mse: 20561.0840 - val_loss: 1768716.5000 - val_mse: 1768716.5000\n",
      "Epoch 677/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 23854.5801 - mse: 23854.5781 - val_loss: 1817298.1250 - val_mse: 1817298.1250\n",
      "Epoch 678/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 23871.3348 - mse: 23871.3340 - val_loss: 1842734.5000 - val_mse: 1842734.5000\n",
      "Epoch 679/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 19008.5923 - mse: 19008.5918 - val_loss: 1860293.5000 - val_mse: 1860293.5000\n",
      "Epoch 680/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 23081.3559 - mse: 23081.3555 - val_loss: 1897856.1250 - val_mse: 1897856.1250\n",
      "Epoch 681/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 26526.4874 - mse: 26526.4883 - val_loss: 1940784.5000 - val_mse: 1940784.5000\n",
      "Epoch 682/1000\n",
      "1948/1948 [==============================] - 0s 13us/sample - loss: 25757.9293 - mse: 25757.9297 - val_loss: 1974823.5000 - val_mse: 1974823.5000\n",
      "Epoch 683/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 26164.9674 - mse: 26164.9668 - val_loss: 1973862.8750 - val_mse: 1973862.8750\n",
      "Epoch 684/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 24110.3808 - mse: 24110.3828 - val_loss: 1998773.0000 - val_mse: 1998773.0000\n",
      "Epoch 685/1000\n",
      "1948/1948 [==============================] - 0s 14us/sample - loss: 21823.4958 - mse: 21823.4941 - val_loss: 1991143.3750 - val_mse: 1991143.3750\n",
      "Epoch 686/1000\n",
      "1948/1948 [==============================] - 0s 15us/sample - loss: 23211.5195 - mse: 23211.5195 - val_loss: 2002147.3750 - val_mse: 2002147.3750\n",
      "Epoch 687/1000\n",
      "1948/1948 [==============================] - 0s 18us/sample - loss: 22141.1016 - mse: 22141.1016 - val_loss: 2001667.3750 - val_mse: 2001667.3750\n",
      "Epoch 688/1000\n",
      "1948/1948 [==============================] - 0s 16us/sample - loss: 26203.8114 - mse: 26203.8105 - val_loss: 1998812.7500 - val_mse: 1998812.7500\n",
      "Epoch 689/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 21230.3925 - mse: 21230.3926 - val_loss: 1989177.6250 - val_mse: 1989177.6250\n",
      "Epoch 690/1000\n",
      "1948/1948 [==============================] - 0s 18us/sample - loss: 20251.9046 - mse: 20251.9062 - val_loss: 1980288.0000 - val_mse: 1980288.0000\n",
      "Epoch 691/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 24553.2440 - mse: 24553.2441 - val_loss: 1982316.3750 - val_mse: 1982316.3750\n",
      "Epoch 692/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 18692.0137 - mse: 18692.0117 - val_loss: 1970762.0000 - val_mse: 1970762.0000\n",
      "Epoch 693/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 26188.3461 - mse: 26188.3457 - val_loss: 1965992.1250 - val_mse: 1965992.1250\n",
      "Epoch 694/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 22955.1995 - mse: 22955.1992 - val_loss: 1941571.0000 - val_mse: 1941571.0000\n",
      "Epoch 695/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 28348.4449 - mse: 28348.4453 - val_loss: 1927794.7500 - val_mse: 1927794.7500\n",
      "Epoch 696/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 23097.0144 - mse: 23097.0137 - val_loss: 1935781.7500 - val_mse: 1935781.7500\n",
      "Epoch 697/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 25005.1861 - mse: 25005.1875 - val_loss: 1936824.1250 - val_mse: 1936824.1250\n",
      "Epoch 698/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 27284.4170 - mse: 27284.4160 - val_loss: 1888866.5000 - val_mse: 1888866.5000\n",
      "Epoch 699/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 20599.4681 - mse: 20599.4688 - val_loss: 1839582.8750 - val_mse: 1839582.8750\n",
      "Epoch 700/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 22661.5858 - mse: 22661.5859 - val_loss: 1820262.8750 - val_mse: 1820262.8750\n",
      "Epoch 701/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 22821.3273 - mse: 22821.3262 - val_loss: 1817286.3750 - val_mse: 1817286.3750\n",
      "Epoch 702/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 31748.8772 - mse: 31748.8789 - val_loss: 1773193.6250 - val_mse: 1773193.6250\n",
      "Epoch 703/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 25697.2543 - mse: 25697.2539 - val_loss: 1739252.7500 - val_mse: 1739252.7500\n",
      "Epoch 704/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 23785.2026 - mse: 23785.2031 - val_loss: 1690403.1250 - val_mse: 1690403.1250\n",
      "Epoch 705/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 20575.4482 - mse: 20575.4492 - val_loss: 1682180.1250 - val_mse: 1682180.1250\n",
      "Epoch 706/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 22065.5915 - mse: 22065.5918 - val_loss: 1699068.7500 - val_mse: 1699068.7500\n",
      "Epoch 707/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 27695.5486 - mse: 27695.5488 - val_loss: 1730025.7500 - val_mse: 1730025.7500\n",
      "Epoch 708/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 26344.6440 - mse: 26344.6445 - val_loss: 1758447.3750 - val_mse: 1758447.3750\n",
      "Epoch 709/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 28928.3446 - mse: 28928.3457 - val_loss: 1785613.8750 - val_mse: 1785613.8750\n",
      "Epoch 710/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 22781.5431 - mse: 22781.5449 - val_loss: 1818273.1250 - val_mse: 1818273.1250\n",
      "Epoch 711/1000\n",
      "1948/1948 [==============================] - 0s 19us/sample - loss: 26241.4296 - mse: 26241.4297 - val_loss: 1836088.3750 - val_mse: 1836088.3750\n",
      "Epoch 712/1000\n",
      "1948/1948 [==============================] - 0s 18us/sample - loss: 26081.8581 - mse: 26081.8574 - val_loss: 1828435.3750 - val_mse: 1828435.3750\n",
      "Epoch 713/1000\n",
      "1948/1948 [==============================] - 0s 18us/sample - loss: 23760.9853 - mse: 23760.9863 - val_loss: 1841784.7500 - val_mse: 1841784.7500\n",
      "Epoch 714/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 27023.1760 - mse: 27023.1738 - val_loss: 1843807.8750 - val_mse: 1843807.8750\n",
      "Epoch 715/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 23397.6558 - mse: 23397.6543 - val_loss: 1825429.2500 - val_mse: 1825429.2500\n",
      "Epoch 716/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 22049.5383 - mse: 22049.5371 - val_loss: 1847775.1250 - val_mse: 1847775.1250\n",
      "Epoch 717/1000\n",
      "1948/1948 [==============================] - 0s 18us/sample - loss: 21898.2290 - mse: 21898.2305 - val_loss: 1853665.5000 - val_mse: 1853665.5000\n",
      "Epoch 718/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 23526.7733 - mse: 23526.7715 - val_loss: 1845612.1250 - val_mse: 1845612.1250\n",
      "Epoch 719/1000\n",
      "1948/1948 [==============================] - 0s 18us/sample - loss: 25091.0564 - mse: 25091.0547 - val_loss: 1788029.2500 - val_mse: 1788029.2500\n",
      "Epoch 720/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 20394.9099 - mse: 20394.9102 - val_loss: 1730922.3750 - val_mse: 1730922.3750\n",
      "Epoch 721/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 20438.1769 - mse: 20438.1758 - val_loss: 1707695.7500 - val_mse: 1707695.7500\n",
      "Epoch 722/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 21974.2885 - mse: 21974.2871 - val_loss: 1713411.3750 - val_mse: 1713411.3750\n",
      "Epoch 723/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 29011.2568 - mse: 29011.2559 - val_loss: 1733866.8750 - val_mse: 1733866.8750\n",
      "Epoch 724/1000\n",
      "1948/1948 [==============================] - 0s 18us/sample - loss: 19651.6578 - mse: 19651.6582 - val_loss: 1741629.0000 - val_mse: 1741629.0000\n",
      "Epoch 725/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 23169.5659 - mse: 23169.5645 - val_loss: 1772783.3750 - val_mse: 1772783.3750\n",
      "Epoch 726/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 24026.7455 - mse: 24026.7461 - val_loss: 1797433.7500 - val_mse: 1797433.7500\n",
      "Epoch 727/1000\n",
      "1948/1948 [==============================] - 0s 16us/sample - loss: 22925.3933 - mse: 22925.3926 - val_loss: 1825812.6250 - val_mse: 1825812.6250\n",
      "Epoch 728/1000\n",
      "1948/1948 [==============================] - 0s 18us/sample - loss: 27041.1671 - mse: 27041.1660 - val_loss: 1855190.7500 - val_mse: 1855190.7500\n",
      "Epoch 729/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 18870.7396 - mse: 18870.7383 - val_loss: 1893958.2500 - val_mse: 1893958.2500\n",
      "Epoch 730/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 27898.5846 - mse: 27898.5840 - val_loss: 1891925.1250 - val_mse: 1891925.1250\n",
      "Epoch 731/1000\n",
      "1948/1948 [==============================] - 0s 16us/sample - loss: 21551.5912 - mse: 21551.5918 - val_loss: 1888711.3750 - val_mse: 1888711.3750\n",
      "Epoch 732/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 22599.8022 - mse: 22599.8027 - val_loss: 1829587.1250 - val_mse: 1829587.1250\n",
      "Epoch 733/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 20215.5306 - mse: 20215.5293 - val_loss: 1782070.2500 - val_mse: 1782070.2500\n",
      "Epoch 734/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 22914.6069 - mse: 22914.6074 - val_loss: 1766520.5000 - val_mse: 1766520.5000\n",
      "Epoch 735/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 23018.6184 - mse: 23018.6172 - val_loss: 1766103.7500 - val_mse: 1766103.7500\n",
      "Epoch 736/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 24301.4177 - mse: 24301.4160 - val_loss: 1764047.6250 - val_mse: 1764047.6250\n",
      "Epoch 737/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 23197.8865 - mse: 23197.8867 - val_loss: 1733280.8750 - val_mse: 1733280.8750\n",
      "Epoch 738/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 24571.1400 - mse: 24571.1387 - val_loss: 1696612.8750 - val_mse: 1696612.8750\n",
      "Epoch 739/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 21112.9782 - mse: 21112.9766 - val_loss: 1672007.3750 - val_mse: 1672007.3750\n",
      "Epoch 740/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 25609.4392 - mse: 25609.4375 - val_loss: 1672487.6250 - val_mse: 1672487.6250\n",
      "Epoch 741/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 22492.7963 - mse: 22492.7969 - val_loss: 1688253.0000 - val_mse: 1688253.0000\n",
      "Epoch 742/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 26032.1719 - mse: 26032.1719 - val_loss: 1732925.7500 - val_mse: 1732925.7500\n",
      "Epoch 743/1000\n",
      "1948/1948 [==============================] - 0s 16us/sample - loss: 25921.9041 - mse: 25921.9062 - val_loss: 1762009.5000 - val_mse: 1762009.5000\n",
      "Epoch 744/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 26419.6987 - mse: 26419.7012 - val_loss: 1762296.1250 - val_mse: 1762296.1250\n",
      "Epoch 745/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 23878.0922 - mse: 23878.0918 - val_loss: 1751070.5000 - val_mse: 1751070.5000\n",
      "Epoch 746/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 25564.2149 - mse: 25564.2129 - val_loss: 1780053.1250 - val_mse: 1780053.1250\n",
      "Epoch 747/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 27702.4217 - mse: 27702.4238 - val_loss: 1849121.5000 - val_mse: 1849121.5000\n",
      "Epoch 748/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 25285.0853 - mse: 25285.0840 - val_loss: 1871756.0000 - val_mse: 1871756.0000\n",
      "Epoch 749/1000\n",
      "1948/1948 [==============================] - 0s 18us/sample - loss: 19195.4834 - mse: 19195.4824 - val_loss: 1863004.6250 - val_mse: 1863004.6250\n",
      "Epoch 750/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 18364.3463 - mse: 18364.3457 - val_loss: 1839691.5000 - val_mse: 1839691.5000\n",
      "Epoch 751/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 22276.1310 - mse: 22276.1309 - val_loss: 1794366.1250 - val_mse: 1794366.1250\n",
      "Epoch 752/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 21576.2236 - mse: 21576.2246 - val_loss: 1800992.8750 - val_mse: 1800992.8750\n",
      "Epoch 753/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 21132.0296 - mse: 21132.0293 - val_loss: 1783071.3750 - val_mse: 1783071.3750\n",
      "Epoch 754/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 24986.9655 - mse: 24986.9648 - val_loss: 1819727.3750 - val_mse: 1819727.3750\n",
      "Epoch 755/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 24558.2650 - mse: 24558.2656 - val_loss: 1834654.8750 - val_mse: 1834654.8750\n",
      "Epoch 756/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 22425.2966 - mse: 22425.2969 - val_loss: 1826088.3750 - val_mse: 1826088.3750\n",
      "Epoch 757/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 22383.7771 - mse: 22383.7773 - val_loss: 1801034.7500 - val_mse: 1801034.7500\n",
      "Epoch 758/1000\n",
      "1948/1948 [==============================] - 0s 17us/sample - loss: 25379.8108 - mse: 25379.8105 - val_loss: 1757998.2500 - val_mse: 1757998.2500\n",
      "Epoch 759/1000\n",
      "1948/1948 [==============================] - 0s 9us/sample - loss: 23125.8867 - mse: 23125.8867\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-82c7c26d77ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m history = model.fit(X_train, y_train, batch_size=1024, epochs=1000, \n\u001b[0;32m----> 2\u001b[0;31m                     validation_split=0.1, verbose=1)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/cs109b/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/cs109b/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs109b/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs109b/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs109b/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs109b/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs109b/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs109b/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs109b/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs109b/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/cs109b/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=1024, epochs=1000, \n",
    "                    validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fc6fc56ddd0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(range(len(y_train)), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fc6fc5dadd0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(range(len(y_train)), pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
